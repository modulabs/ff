{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "StackNet_test.ipynb",
      "version": "0.3.2",
      "views": {},
      "default_view": {},
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "-QL_vZd1Cdva",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Example based on twosigma competition"
      ]
    },
    {
      "metadata": {
        "id": "mwzDhcVoCf1O",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This example use StackNet along with some preparation with python along with [xgboost](https://github.com/dmlc/xgboost) to score 0.53776 (logloss) for the [twosigma](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries) challenge on kaggle\n",
        "\n",
        "전체 데이터셋을 테스트하는데 시간이 걸려, train/test 500/500 개의 샘플로 테스트함\n",
        "\n",
        "To run follow the next steps:"
      ]
    },
    {
      "metadata": {
        "id": "XJAjvQRryl1S",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "references :\n",
        "* https://github.com/kaz-Anova/StackNet\n",
        "* https://github.com/kaz-Anova/StackNet/tree/master/example/twosigma_kaggle\n",
        "* https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries#description"
      ]
    },
    {
      "metadata": {
        "id": "e34-62AWCz5z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 1.Download the train.json and test.json files from [here](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/data)"
      ]
    },
    {
      "metadata": {
        "id": "h6lKocZpuz14",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "4ff22df8-96d4-4477-8003-2ae620d7ecfb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525050507826,
          "user_tz": -540,
          "elapsed": 4228,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install kaggle"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://legacy.pypi.org/simple\n",
            "Collecting kaggle\n",
            "  Downloading https://files.pythonhosted.org/packages/db/a5/f7cc3600bcec5adf9ac82e44a382385d3c47f7d8d9c580d4a10c36d231a3/kaggle-1.3.3.tar.gz\n",
            "Requirement already satisfied: urllib3>=1.15 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.22)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.11.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2018.4.16)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.5.3)\n",
            "Building wheels for collected packages: kaggle\n",
            "  Running setup.py bdist_wheel for kaggle ... \u001b[?25l-\b \b\\\b \bdone\n",
            "\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/dc/03/02/372e828042d91dc9d96c6d5eab5b14591790dafe70600b7309\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "Successfully installed kaggle-1.3.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "m9JFjy2s203W",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!mkdir ~/.kaggle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tVdWEHHEvcIL",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!echo \"API JSON\" > ~/.kaggle/kaggle.json\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kdXytpOZvpvZ",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "8d7725b9-0675-4931-879a-ca4cae6140bb",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525050521525,
          "user_tz": -540,
          "elapsed": 6407,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c \"two-sigma-connect-rental-listing-inquiries\" -f \"train.json.zip\" \n",
        "!kaggle competitions download -c \"two-sigma-connect-rental-listing-inquiries\" -f \"test.json.zip\""
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train.json.zip: Downloaded 20MB of 20MB\n",
            "test.json.zip: Downloaded 30MB of 30MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f0SGYGNBwDV6",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "feb2107f-3266-466c-fb67-170914c84f1c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525050523607,
          "user_tz": -540,
          "elapsed": 2021,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al .kaggle/competitions/two-sigma-connect-rental-listing-inquiries"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 51236\r\n",
            "drwxr-xr-x 2 root root     4096 Apr 30 01:08 .\r\n",
            "drwxr-xr-x 3 root root     4096 Apr 30 01:08 ..\r\n",
            "-rw-r--r-- 1 root root 31487242 Apr 30 01:08 test.json.zip\r\n",
            "-rw-r--r-- 1 root root 20965425 Apr 30 01:08 train.json.zip\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pr0w4RzowGzS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "39a62d97-5f21-4fa0-fed2-5214d1db4007",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525050552922,
          "user_tz": -540,
          "elapsed": 5265,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!unzip .kaggle/competitions/two-sigma-connect-rental-listing-inquiries/train.json.zip -d ~/input/\n",
        "!unzip .kaggle/competitions/two-sigma-connect-rental-listing-inquiries/test.json.zip -d ~/input/"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  .kaggle/competitions/two-sigma-connect-rental-listing-inquiries/train.json.zip\n",
            "  inflating: /content/input/train.json  \n",
            "Archive:  .kaggle/competitions/two-sigma-connect-rental-listing-inquiries/test.json.zip\n",
            "  inflating: /content/input/test.json  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "92w9y8-FFDsd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2. Download the StackNet.jar file from the Git"
      ]
    },
    {
      "metadata": {
        "id": "b0iH4zI-tGia",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "aabbf736-05e9-4ef9-f70b-8c86210f60a5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525050547637,
          "user_tz": -540,
          "elapsed": 23990,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/HwangJohn/StackNet"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'StackNet'...\n",
            "remote: Counting objects: 1362, done.\u001b[K\n",
            "remote: Compressing objects: 100% (7/7), done.\u001b[K\n",
            "remote: Total 1362 (delta 1), reused 4 (delta 1), pack-reused 1354\u001b[K\n",
            "Receiving objects: 100% (1362/1362), 81.86 MiB | 34.21 MiB/s, done.\n",
            "Resolving deltas: 100% (671/671), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "XqB1nmEExc-D",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "outputId": "c813ec12-5534-4d3c-8f02-862e1361b3da",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525050554753,
          "user_tz": -540,
          "elapsed": 1809,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 48\r\n",
            "drwxr-xr-x  1 root root 4096 Apr 30 01:09 .\r\n",
            "drwxr-xr-x  1 root root 4096 Apr 30 00:50 ..\r\n",
            "drwx------  4 root root 4096 Apr 30 00:51 .cache\r\n",
            "drwxr-xr-x  3 root root 4096 Apr 30 00:51 .config\r\n",
            "drwxr-xr-x  1 root root 4096 Apr 23 20:19 datalab\r\n",
            "drwxr-xr-x  4 root root 4096 Apr 30 00:51 .forever\r\n",
            "drwxr-xr-x  2 root root 4096 Apr 30 01:09 input\r\n",
            "drwxr-xr-x  5 root root 4096 Apr 30 00:51 .ipython\r\n",
            "drwxr-xr-x  3 root root 4096 Apr 30 01:08 .kaggle\r\n",
            "drwx------  3 root root 4096 Apr 30 00:51 .local\r\n",
            "-rw-------  1 root root 1024 Apr 30 00:51 .rnd\r\n",
            "drwxr-xr-x 11 root root 4096 Apr 30 01:09 StackNet\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B-O8cdHFFOOS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 3. Ensure you have Python installed along with a fairly recent version of xgboost"
      ]
    },
    {
      "metadata": {
        "id": "YWfUNE8yFPSC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 4. Make certain you have Java higher than 1.6 installed and that Java is in your PATH. Have a look at [this](https://www.java.com/en/download/help/path.xml) if you encounter trouble."
      ]
    },
    {
      "metadata": {
        "id": "8eyhjfq7xzQA",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q9S2YiXPxiAK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "f6d06fd2-cae0-4e01-f418-0f54f093dd00",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525050611454,
          "user_tz": -540,
          "elapsed": 1798,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!java -version"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "openjdk version \"1.8.0_162\"\r\n",
            "OpenJDK Runtime Environment (build 1.8.0_162-8u162-b12-0ubuntu0.17.10.2-b12)\r\n",
            "OpenJDK 64-Bit Server VM (build 25.162-b12, mixed mode)\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ol64-ST-yXO4",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 137
        },
        "outputId": "759ef0bd-a97d-4ffc-bee1-708aac767a9e",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525050613911,
          "user_tz": -540,
          "elapsed": 2310,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al StackNet/example/twosigma_kaggle"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 32\r\n",
            "drwxr-xr-x 2 root root  4096 Apr 30 01:09 .\r\n",
            "drwxr-xr-x 9 root root  4096 Apr 30 01:09 ..\r\n",
            "-rw-r--r-- 1 root root 15824 Apr 30 01:09 create_files_v1.py\r\n",
            "-rw-r--r-- 1 root root  2656 Apr 30 01:09 EXAMPLE.MD\r\n",
            "-rw-r--r-- 1 root root  1805 Apr 30 01:09 paramssimplev1.txt\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zmDBDFSgFaWp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 5. Run the create_files_v1.py script (also in the example section of the git). This is based on SRK’s starter python script . It creates some data from the raw json files, runs an xgboost (which scores around 0.544 to LB)and stacks xgboost’s predictions onto the data and then prints them out as csvs (e.g. train_stacknet.csv and test_stacknet.csv) . The intuition is mostly speed as well as the fact that xgboost is too good and since it is not part of the initial models on StackNet – I had to somehow add it!"
      ]
    },
    {
      "metadata": {
        "id": "Dnx7Vd4zyjLW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 830
        },
        "outputId": "a487ad94-b311-479e-cc85-4c0c3afc6a5c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525053940990,
          "user_tz": -540,
          "elapsed": 345472,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!chmod 755 -R StackNet\n",
        "!python2 StackNet/example/twosigma_kaggle/create_files_v1.py"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\r\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "(49352, 15)\n",
            "(74659, 14)\n",
            "display_address 16068\n",
            "manager_id 4399\n",
            "building_id 11635\n",
            "street_address 25766\n",
            "num_furniture 19\n",
            "display_address_manager_id 62997\n",
            "display_address_building_id 28492\n",
            "display_address_street_address 29336\n",
            "display_address_num_furniture 25625\n",
            "manager_id_building_id 58550\n",
            "manager_id_street_address 73650\n",
            "manager_id_num_furniture 15233\n",
            "building_id_street_address 29348\n",
            "building_id_num_furniture 19148\n",
            "street_address_num_furniture 37085\n",
            "10                                                         \n",
            "10000     Doorman Elevator Fitness_Center Cats_Allowed D...\n",
            "100004    Laundry_In_Building Dishwasher Hardwood_Floors...\n",
            "100007                               Hardwood_Floors No_Fee\n",
            "100013                                              Pre-War\n",
            "Name: features, dtype: object\n",
            "['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price', 'num_photos', 'num_features', 'num_description_words', 'created_month', 'created_day', 'listing_id', 'created_hour', 'total_days', 'diff_rank', 'num_price_by_furniture', 'price_latitue', 'price_longtitude', 'display_address', 'manager_id', 'building_id', 'street_address', 'num_furniture', 'display_address_manager_id', 'display_address_building_id', 'display_address_street_address', 'display_address_num_furniture', 'manager_id_building_id', 'manager_id_street_address', 'manager_id_num_furniture', 'building_id_street_address', 'building_id_num_furniture', 'street_address_num_furniture']\n",
            "((49352, 282), (74659, 282))\n",
            "((500, 282), (500, 282))\n",
            "scalling\n",
            "kfolder\n",
            "starting cross validation with 5 kfolds \n",
            " train size: 399. test size: 101, cols: 285 \n",
            "size train: 399 size cv: 101 loglikelihood (fold 1/5): 0.691321\n",
            " train size: 400. test size: 100, cols: 285 \n",
            "size train: 400 size cv: 100 loglikelihood (fold 2/5): 0.682323\n",
            " train size: 400. test size: 100, cols: 285 \n",
            "size train: 400 size cv: 100 loglikelihood (fold 3/5): 0.766278\n",
            " train size: 400. test size: 100, cols: 285 \n",
            "size train: 400 size cv: 100 loglikelihood (fold 4/5): 0.738935\n",
            " train size: 401. test size: 99, cols: 285 \n",
            "size train: 401 size cv: 99 loglikelihood (fold 5/5): 0.723723\n",
            " Average Lolikelihood: 0.720516\n",
            " making test predictions \n",
            "merging columns\n",
            "exporting files\n",
            "Write results...\n",
            "Writing submission to submission_0.7205158757115696.csv\n",
            "Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0cV0-EKGywvT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "90854118-b129-4885-b52f-c2aaaa9d8ff0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525054890789,
          "user_tz": -540,
          "elapsed": 1846,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 2448\r\n",
            "drwxr-xr-x  1 root root    4096 Apr 30 01:15 .\r\n",
            "drwxr-xr-x  1 root root    4096 Apr 30 00:50 ..\r\n",
            "drwx------  4 root root    4096 Apr 30 00:51 .cache\r\n",
            "drwxr-xr-x  3 root root    4096 Apr 30 00:51 .config\r\n",
            "drwxr-xr-x  1 root root    4096 Apr 23 20:19 datalab\r\n",
            "drwxr-xr-x  4 root root    4096 Apr 30 00:51 .forever\r\n",
            "drwxr-xr-x  2 root root    4096 Apr 30 01:09 input\r\n",
            "drwxr-xr-x  5 root root    4096 Apr 30 00:51 .ipython\r\n",
            "drwxr-xr-x  3 root root    4096 Apr 30 01:08 .kaggle\r\n",
            "drwx------  3 root root    4096 Apr 30 00:51 .local\r\n",
            "-rw-------  1 root root    1024 Apr 30 00:51 .rnd\r\n",
            "drwxr-xr-x 11 root root    4096 Apr 30 01:09 StackNet\r\n",
            "-rw-r--r--  1 root root   17527 Apr 30 02:05 submission_0.7205158757115696.csv\r\n",
            "-rw-r--r--  1 root root 1217436 Apr 30 02:05 test_stacknet.csv\r\n",
            "-rw-r--r--  1 root root 1214321 Apr 30 02:05 train_stacknet.csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F3Eja7TkFqHE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 6. Put the jar file, the attached parameters’ file (paramssimplev1.txt) , the printed csv files into the same folder. The parameter files contains 9 level 1 models and 1 level 2 model. All models within the level are built in parallel."
      ]
    },
    {
      "metadata": {
        "id": "MCofTcfeimhT",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "outputId": "ddf2ffc4-f68b-4ce0-b6cb-f6f7386b18d5",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525054880538,
          "user_tz": -540,
          "elapsed": 1892,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!cat StackNet/example/twosigma_kaggle/paramssimplev1.txt"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression Type:Liblinear C:5.0 threads:1 usescale:True maxim_Iteration:200 seed:1 verbose:false\r\r\n",
            "GradientBoostingForestClassifier estimators:300 shrinkage:0.1 threads:1 offset:0.00001 max_depth:3 max_features:0.4 min_leaf:2.0 min_split:5.0 Objective:RMSE row_subsample:0.7 seed:1 verbose:false\r\r\n",
            "LibFmClassifier maxim_Iteration:70 C:0.0001 C2:0.0009 lfeatures:2 seed:1 usescale:True init_values:0.1 learn_rate:0.1 smooth:0.01 threads:1 verbose:false\r\r\n",
            "softmaxnnclassifier usescale:True seed:1 Type:SGD maxim_Iteration:50 C:0.000001 shuffle:false tolerance:0.01 learn_rate:0.01 smooth:0.1 h1:20 h2:20 connection_nonlinearity:Relu init_values:0.02 verbose:false\r\r\n",
            "RandomForestClassifier bootsrap:false estimators:100 threads:1 offset:0.00001 max_depth:6 max_features:0.4 min_leaf:2.0 min_split:5.0 Objective:ENTROPY row_subsample:0.95 seed:1 verbose:false\r\r\n",
            "AdaboostRandomForestClassifier bootsrap:false weight_thresold:0.95 estimators:100 threads:1 max_depth:6 max_features:0.5 min_leaf:2.0 min_split:5.0 Objective:ENTROPY row_subsample:0.9 seed:1 verbose:false\r\r\n",
            "GradientBoostingForestRegressor bootsrap:false estimators:300 shrinkage:0.1 threads:1 offset:0.00001 max_depth:3 max_features:0.4 min_leaf:2.0 min_split:5.0 Objective:RMSE row_subsample:0.9 seed:1 verbose:false\r\r\n",
            "RandomForestRegressor bootsrap:false estimators:100 threads:1 offset:0.00001 max_depth:6 max_features:0.4 min_leaf:2.0 min_split:5.0 Objective:RMSE row_subsample:0.95 seed:1 verbose:false\r\r\n",
            "LibFmRegressor maxim_Iteration:70 C:0.0001 C2:0.0009 lfeatures:2 seed:1 usescale:True init_values:0.1 learn_rate:0.1 threads:1 verbose:false\r\r\n",
            "\r\r\n",
            "RandomForestClassifier bootsrap:false estimators:1000 threads:3 offset:0.00001 max_depth:5 max_features:0.3 min_leaf:1.0 min_split:5.0 Objective:ENTROPY row_subsample:0.8 seed:1 verbose:false\r\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "upsvH5RBW2f9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 7. Run on the command line. You may need to look on the parameters’ section on GitHub to understand what is everything. CD to that folder while in command line and press: java -Xmx3048m -jar StackNet.jar train task=classification train_file=train_stacknet.csv test_file=test_stacknet.csv params=paramssimplev1.txt pred_file=sigma_stack_pred.csv test_target=true verbose=true Threads=3 stackdata=false folds=5 seed=1 metric=logloss"
      ]
    },
    {
      "metadata": {
        "id": "Aa6MUlZCF51y",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!cp StackNet/example/twosigma_kaggle/paramssimplev1.txt .\n",
        "!cp StackNet/StackNet.jar ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JiaarCPLXF1V",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 3689
        },
        "outputId": "b1db7ec0-2b22-49c0-c615-53e4c8077bd6",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525055291883,
          "user_tz": -540,
          "elapsed": 255568,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!java -Xmx3048m -jar StackNet.jar train task=classification train_file=train_stacknet.csv test_file=test_stacknet.csv params=paramssimplev1.txt pred_file=sigma_stack_pred.csv test_target=true verbose=true Threads=3 stackdata=false folds=5 seed=1 metric=logloss"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameter name : task value :  classification\r\n",
            "parameter name : train_file value :  train_stacknet.csv\r\n",
            "parameter name : test_file value :  test_stacknet.csv\r\n",
            "parameter name : params value :  paramssimplev1.txt\r\n",
            "parameter name : pred_file value :  sigma_stack_pred.csv\r\n",
            "parameter name : test_target value :  true\r\n",
            "parameter name : verbose value :  true\r\n",
            "parameter name : threads value :  3\r\n",
            "parameter name : stackdata value :  false\r\n",
            "parameter name : folds value :  5\r\n",
            "parameter name : seed value :  1\r\n",
            "parameter name : metric value :  logloss\n",
            " Completed: 5.00 % \n",
            " Completed: 10.00 % \n",
            " Completed: 15.00 % \n",
            " Completed: 20.00 % \n",
            " Completed: 25.00 % \n",
            " Completed: 30.00 % \n",
            " Completed: 35.00 % \n",
            " Completed: 40.00 % \n",
            " Completed: 45.00 % \n",
            " Completed: 50.00 % \n",
            " Completed: 55.00 % \n",
            " Completed: 60.00 % \n",
            " Completed: 65.00 % \n",
            " Completed: 70.00 % \n",
            " Completed: 75.00 % \n",
            " Completed: 80.00 % \n",
            " Completed: 85.00 % \n",
            " Completed: 90.00 % \n",
            " Completed: 95.00 % \n",
            " Completed: 100.00 % \n",
            " Loaded File: train_stacknet.csv\n",
            " Total rows in the file: 500\n",
            " Total columns in the file: 286\n",
            " Weighted variable : -1 counts: 0\n",
            " Int Id variable : -1 str id: -1 counts: 0\n",
            " Target Variables  : 1 values : [0]\n",
            " Actual columns number  : 285\n",
            " Number of Skipped rows   : 0\n",
            " Actual Rows (removing the skipped ones)  : 500\n",
            "Loaded dense train data with 500 and columns 285\n",
            " loaded data in : 0.269000\n",
            " Level: 1 dimensionality: 21\n",
            " Starting cross validation \n",
            "Fitting model: 1\n",
            "Fitting model: 2\n",
            "Fitting model: 3\n",
            " logloss : 0.7760798699519157\n",
            " logloss : 0.76718394997182\n",
            " logloss : 1.1723773262528183\n",
            "Fitting model: 4\n",
            "Fitting model: 5\n",
            "Fitting model: 6\n",
            " logloss : 0.7696911978838683\n",
            " logloss : 0.7922018387816844\n",
            " logloss : 0.7959157020622684\n",
            "Fitting model: 7\n",
            "Fitting model: 8\n",
            "Fitting model: 9\n",
            " rmse : 0.5716674345620296\n",
            " rmse : 0.5772137893645404\n",
            " rmse : 2.773254222815028\n",
            "Done with fold: 1/5\n",
            "Fitting model: 1\n",
            "Fitting model: 2\n",
            "Fitting model: 3\n",
            " logloss : 0.729877923368898\n",
            " logloss : 0.7943920205762438\n",
            " logloss : 1.2338023389431305\n",
            "Fitting model: 4\n",
            "Fitting model: 5\n",
            "Fitting model: 6\n",
            " logloss : 0.7426942230528703\n",
            " logloss : 0.7132925242517721\n",
            " logloss : 0.7244175920141532\n",
            "Fitting model: 7\n",
            "Fitting model: 8\n",
            "Fitting model: 9\n",
            " rmse : 0.608295114865718\n",
            " rmse : 0.5912204554496382\n",
            " rmse : 1475.8734005579859\n",
            "Done with fold: 2/5\n",
            "Fitting model: 1\n",
            "Fitting model: 2\n",
            "Fitting model: 3\n",
            " logloss : 0.7084650027202057\n",
            " logloss : 0.6334118656943822\n",
            " logloss : NaN\n",
            "Fitting model: 4\n",
            "Fitting model: 5\n",
            "Fitting model: 6\n",
            " logloss : 0.6701095319241038\n",
            " logloss : 0.634963139140524\n",
            " logloss : 0.6361491030902237\n",
            "Fitting model: 7\n",
            "Fitting model: 8\n",
            "Fitting model: 9\n",
            " rmse : 0.548422868304526\n",
            " rmse : 0.5501959950185434\n",
            " rmse : 571.0295002517723\n",
            "Done with fold: 3/5\n",
            "Fitting model: 1\n",
            "Fitting model: 2\n",
            "Fitting model: 3\n",
            " logloss : 0.7386716286590135\n",
            " logloss : 0.7017277029421473\n",
            " logloss : 1.3833432854077279\n",
            "Fitting model: 4\n",
            "Fitting model: 5\n",
            "Fitting model: 6\n",
            " logloss : 0.7093909281725508\n",
            " logloss : 0.6959857969445902\n",
            " logloss : 0.7032336844416718\n",
            "Fitting model: 7\n",
            "Fitting model: 8\n",
            "Fitting model: 9\n",
            " rmse : 0.5461190171672967\n",
            " rmse : 0.5417842884923406\n",
            " rmse : 61.90473109085714\n",
            "Done with fold: 4/5\n",
            "Fitting model: 1\n",
            "Fitting model: 2\n",
            "Fitting model: 3\n",
            " logloss : 0.7655681535238286\n",
            " logloss : 0.9200232379822043\n",
            " logloss : NaN\n",
            "Fitting model: 4\n",
            "Fitting model: 5\n",
            "Fitting model: 6\n",
            " logloss : 0.6945946571023087\n",
            " logloss : 0.7802422508952925\n",
            " logloss : 0.7729408044241076\n",
            "Fitting model: 7\n",
            "Fitting model: 8\n",
            "Fitting model: 9\n",
            " rmse : 0.6180534770583578\n",
            " rmse : 0.5588015176994393\n",
            " rmse : 2750.9593809449802\n",
            "Done with fold: 5/5\n",
            " Average of all folds model 0 : 0.7437325156447724\n",
            " Average of all folds model 1 : 0.7633477554333595\n",
            " Average of all folds model 2 : NaN\n",
            " Average of all folds model 3 : 0.7172961076271405\n",
            " Average of all folds model 4 : 0.7233371100027727\n",
            " Average of all folds model 5 : 0.726531377206485\n",
            " Average of all folds model 6 : 0.5785115823915856\n",
            " Average of all folds model 7 : 0.5638432092049004\n",
            " Average of all folds model 8 : 972.5080534136821\n",
            " Level: 1 start output modelling \n",
            "Fitting model : 1\n",
            "Fitting model : 2\n",
            "Fitting model : 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Fitting model : 4\r\n",
            "Fitting model : 5\r\n",
            "Fitting model : 6\n",
            "Fitting model : 7\n",
            "Fitting model : 8\n",
            "Fitting model : 9\n",
            "Completed level: 1 out of 2\n",
            " Level: 2 dimensionality: 3\n",
            " Starting cross validation \n",
            " Average of all folds model 0 : 0.0\n",
            " Level: 2 start output modelling \n",
            "Fitting model : 1\n",
            "Completed level: 2 out of 2\n",
            " modelling lasted : 251.265000\n",
            " Completed: 5.00 % \n",
            " Completed: 10.00 % \n",
            " Completed: 15.00 % \n",
            " Completed: 20.00 % \n",
            " Completed: 25.00 % \n",
            " Completed: 30.00 % \n",
            " Completed: 35.00 % \n",
            " Completed: 40.00 % \n",
            " Completed: 45.00 % \n",
            " Completed: 50.00 % \n",
            " Completed: 55.00 % \n",
            " Completed: 60.00 % \n",
            " Completed: 65.00 % \n",
            " Completed: 70.00 % \n",
            " Completed: 75.00 % \n",
            " Completed: 80.00 % \n",
            " Completed: 85.00 % \n",
            " Completed: 90.00 % \n",
            " Completed: 95.00 % \n",
            " Completed: 100.00 % \n",
            " Loaded File: test_stacknet.csv\n",
            " Total rows in the file: 500\n",
            " Total columns in the file: 286\n",
            " Weighted variable : -1 counts: 0\n",
            " Int Id variable : -1 str id: -1 counts: 0\n",
            " Target Variables  : 1 values : [0]\n",
            " Actual columns number  : 285\n",
            " Number of Skipped rows   : 0\n",
            " Actual Rows (removing the skipped ones)  : 500\n",
            "Loaded dense test data with 500 and columns 285\n",
            " loading test data lasted : 0.114000\n",
            " Completed: 5.00 % \n",
            " Completed: 10.00 % \n",
            " Completed: 15.00 % \n",
            " Completed: 20.00 % \n",
            " Completed: 25.00 % \n",
            " Completed: 30.00 % \n",
            " Completed: 35.00 % \n",
            " Completed: 40.00 % \n",
            " Completed: 45.00 % \n",
            " Completed: 50.00 % \n",
            " Completed: 55.00 % \n",
            " Completed: 60.00 % \n",
            " Completed: 65.00 % \n",
            " Completed: 70.00 % \n",
            " Completed: 75.00 % \n",
            " Completed: 80.00 % \n",
            " Completed: 85.00 % \n",
            " Completed: 90.00 % \n",
            " Completed: 95.00 % \n",
            " Completed: 100.00 % \n",
            " predicting on test data lasted : 0.847000\n",
            "Metric could not be calculated on the test \n",
            " The whole StackNet procedure lasted: 253.566000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "t7xPxjbbXt_V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 8. The test_stacknet.csv’s first column is the id. Append it next to the predicted probabilities as they come out from StackNet (sigma_stack_pred.csv) and submit . It should score 0.53776 ."
      ]
    },
    {
      "metadata": {
        "id": "XyKfF4VtXMwD",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "992134c5-cf69-420d-e6a7-f794f38c3896",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525055351963,
          "user_tz": -540,
          "elapsed": 1838,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!ls -al"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 26740\r\n",
            "drwxr-xr-x  1 root root     4096 Apr 30 02:28 .\r\n",
            "drwxr-xr-x  1 root root     4096 Apr 30 00:50 ..\r\n",
            "drwx------  4 root root     4096 Apr 30 00:51 .cache\r\n",
            "drwxr-xr-x  3 root root     4096 Apr 30 00:51 .config\r\n",
            "drwxr-xr-x  1 root root     4096 Apr 23 20:19 datalab\r\n",
            "drwxr-xr-x  4 root root     4096 Apr 30 00:51 .forever\r\n",
            "drwxr-xr-x  2 root root     4096 Apr 30 01:09 input\r\n",
            "drwxr-xr-x  5 root root     4096 Apr 30 00:51 .ipython\r\n",
            "drwxr-xr-x  3 root root     4096 Apr 30 01:08 .kaggle\r\n",
            "drwx------  3 root root     4096 Apr 30 00:51 .local\r\n",
            "-rwxr-xr-x  1 root root     1805 Apr 30 02:23 paramssimplev1.txt\r\n",
            "-rw-------  1 root root     1024 Apr 30 00:51 .rnd\r\n",
            "-rw-r--r--  1 root root    29349 Apr 30 02:28 sigma_stack_pred.csv\r\n",
            "drwxr-xr-x 11 root root     4096 Apr 30 01:09 StackNet\r\n",
            "-rwxr-xr-x  1 root root 12787950 Apr 30 02:23 StackNet.jar\r\n",
            "-rw-r--r--  1 root root 12045501 Apr 30 02:28 stacknet.model\r\n",
            "-rw-r--r--  1 root root    17527 Apr 30 02:05 submission_0.7205158757115696.csv\r\n",
            "-rw-r--r--  1 root root  1217436 Apr 30 02:05 test_stacknet.csv\r\n",
            "-rw-r--r--  1 root root  1214321 Apr 30 02:05 train_stacknet.csv\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TUCwe7rIaVUz",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "dd9b0134-7217-416a-a90e-9d5e11dd2058",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525056141375,
          "user_tz": -540,
          "elapsed": 1050,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_stacknet = pd.read_csv(\"test_stacknet.csv\", sep=\",\", header=-1)\n",
        "test_stacknet.head(1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7142618.0</td>\n",
              "      <td>-0.36262</td>\n",
              "      <td>-0.46231</td>\n",
              "      <td>-0.61286</td>\n",
              "      <td>-0.43427</td>\n",
              "      <td>-0.2537</td>\n",
              "      <td>0.55198</td>\n",
              "      <td>0.13305</td>\n",
              "      <td>-0.20004</td>\n",
              "      <td>2.78782</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.04477</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.04477</td>\n",
              "      <td>-0.04477</td>\n",
              "      <td>0.76316</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.18104</td>\n",
              "      <td>0.06423</td>\n",
              "      <td>0.61788</td>\n",
              "      <td>0.31789</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1 rows × 286 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0        1        2        3        4       5        6        7    \\\n",
              "0  7142618.0 -0.36262 -0.46231 -0.61286 -0.43427 -0.2537  0.55198  0.13305   \n",
              "\n",
              "       8        9     ...         276  277      278      279      280  281  \\\n",
              "0 -0.20004  2.78782   ...    -0.04477  0.0 -0.04477 -0.04477  0.76316  0.0   \n",
              "\n",
              "       282      283      284      285  \n",
              "0 -0.18104  0.06423  0.61788  0.31789  \n",
              "\n",
              "[1 rows x 286 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "metadata": {
        "id": "Xvbk7R1-Yig-",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "4360b383-f69c-4eff-e43f-5b12fe1d2375",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525056217694,
          "user_tz": -540,
          "elapsed": 934,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "sigma_stack_pred = pd.read_csv(\"sigma_stack_pred.csv\", sep=\",\", header=-1)\n",
        "sigma_stack_pred.head(1)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.162921</td>\n",
              "      <td>0.415502</td>\n",
              "      <td>0.421577</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2\n",
              "0  0.162921  0.415502  0.421577"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "metadata": {
        "id": "XMw0sm4Aa1OU",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "s1 = pd.Series(test_stacknet.iloc[0], name='id')\n",
        "result = pd.concat([s1, sigma_stack_pred], axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R_xaMOfVYh8V",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 99
        },
        "outputId": "115cce4a-d081-421f-8b89-3e61309f8bda",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525056397166,
          "user_tz": -540,
          "elapsed": 1033,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "result.head(1)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>7142618.0</td>\n",
              "      <td>0.162921</td>\n",
              "      <td>0.415502</td>\n",
              "      <td>0.421577</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          id         0         1         2\n",
              "0  7142618.0  0.162921  0.415502  0.421577"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "53lzEqO2iIcB",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# 샘플데이터이기 때문에 kaggle에서는 확인이 힘듬\n",
        "from sklearn.metrics import log_loss\n",
        "\n",
        "log_loss(y_true, y_pred, eps=1e-15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TtrDKrJnczuT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### [KerasnnClassifier](https://github.com/kaz-Anova/StackNet/blob/master/parameters/PARAMETERS.MD#kerasnnclassifier)\n",
        "\n",
        "The original parameters can be found sparsely in [keras' documentation](https://keras.io/)\n",
        "\n",
        "KerasnnClassifier \n",
        "  * loss:categorical_crossentropy \n",
        "  * standardize:true \n",
        "  * use_log1p:true \n",
        "  * shuffle:true \n",
        "  * batch_normalization:true \n",
        "  * weight_init:lecun_uniform \n",
        "  * momentum:0.9 \n",
        "  * optimizer:adam \n",
        "  * use_dense:true \n",
        "  * l2:0.000001,0.000001 \n",
        "  * hidden:50,50 \n",
        "  * activation:relu,relu \n",
        "  * droupouts:0.1,0.1 \n",
        "  * epochs:20 lr:0.01 \n",
        "  * batch_size:8 \n",
        "  * stopping_rounds:10 \n",
        "  * validation_split:0.2 \n",
        "  * seed:1 \n",
        "  * verbose:false"
      ]
    },
    {
      "metadata": {
        "id": "gf7ztr9yaDVW",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!echo \"LogisticRegression Type:Liblinear C:5.0 threads:1 usescale:True maxim_Iteration:200 seed:1 verbose:false\\n\\\n",
        "GradientBoostingForestClassifier estimators:300 shrinkage:0.1 threads:1 offset:0.00001 max_depth:3 max_features:0.4 min_leaf:2.0 min_split:5.0 Objective:RMSE row_subsample:0.7 seed:1 verbose:false\\n\\\n",
        "KerasnnClassifier loss:categorical_crossentropy standardize:true use_log1p:true shuffle:true batch_normalization:true weight_init:lecun_uniform momentum:0.9 optimizer:adam use_dense:true l2:0.000001,0.000001 hidden:50,50 activation:relu,relu droupouts:0.1,0.1 epochs:20 lr:0.01 batch_size:8 stopping_rounds:10 validation_split:0.2 seed:1 verbose:true\\n\\\n",
        "\\n\\\n",
        "RandomForestClassifier bootsrap:false estimators:1000 threads:3 offset:0.00001 max_depth:5 max_features:0.3 min_leaf:1.0 min_split:5.0 Objective:ENTROPY row_subsample:0.8 seed:1 verbose:false\" > paramssimplev2.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "nIssZDIgi8IS",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "7910fa0e-a37c-4d69-c454-44f746b0329c",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525061093014,
          "user_tz": -540,
          "elapsed": 1763,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!cat paramssimplev2.txt"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LogisticRegression Type:Liblinear C:5.0 threads:1 usescale:True maxim_Iteration:200 seed:1 verbose:false\r\n",
            "GradientBoostingForestClassifier estimators:300 shrinkage:0.1 threads:1 offset:0.00001 max_depth:3 max_features:0.4 min_leaf:2.0 min_split:5.0 Objective:RMSE row_subsample:0.7 seed:1 verbose:false\r\n",
            "KerasnnClassifier loss:categorical_crossentropy standardize:true use_log1p:true shuffle:true batch_normalization:true weight_init:lecun_uniform momentum:0.9 optimizer:adam use_dense:true l2:0.000001,0.000001 hidden:50,50 activation:relu,relu droupouts:0.1,0.1 epochs:20 lr:0.01 batch_size:8 stopping_rounds:10 validation_split:0.2 seed:1 verbose:true\r\n",
            "\r\n",
            "RandomForestClassifier bootsrap:false estimators:1000 threads:3 offset:0.00001 max_depth:5 max_features:0.3 min_leaf:1.0 min_split:5.0 Objective:ENTROPY row_subsample:0.8 seed:1 verbose:false\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aRtXmWbsv2jK",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          }
        }
      },
      "cell_type": "code",
      "source": [
        "# Copy Generic Algorithms in current path (Install Python Generic Algorithms)\n",
        "!cp -r StackNet/lib ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "H4HkB-wjip9F",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 15353
        },
        "outputId": "73ad91f3-b381-43ba-abf4-7b96f66d05e7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525061938544,
          "user_tz": -540,
          "elapsed": 121130,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!java -Xmx3048m -jar StackNet.jar train task=classification train_file=train_stacknet.csv test_file=test_stacknet.csv params=paramssimplev2.txt pred_file=sigma_stack_pred2.csv test_target=true verbose=true Threads=3 stackdata=false folds=5 seed=1 metric=logloss"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "parameter name : task value :  classification\r\n",
            "parameter name : train_file value :  train_stacknet.csv\r\n",
            "parameter name : test_file value :  test_stacknet.csv\r\n",
            "parameter name : params value :  paramssimplev2.txt\r\n",
            "parameter name : pred_file value :  sigma_stack_pred2.csv\r\n",
            "parameter name : test_target value :  true\r\n",
            "parameter name : verbose value :  true\r\n",
            "parameter name : threads value :  3\r\n",
            "parameter name : stackdata value :  false\r\n",
            "parameter name : folds value :  5\r\n",
            "parameter name : seed value :  1\r\n",
            "parameter name : metric value :  logloss\n",
            " Completed: 5.00 % \n",
            " Completed: 10.00 % \n",
            " Completed: 15.00 % \n",
            " Completed: 20.00 % \n",
            " Completed: 25.00 % \n",
            " Completed: 30.00 % \n",
            " Completed: 35.00 % \n",
            " Completed: 40.00 % \n",
            " Completed: 45.00 % \n",
            " Completed: 50.00 % \n",
            " Completed: 55.00 % \n",
            " Completed: 60.00 % \n",
            " Completed: 65.00 % \n",
            " Completed: 70.00 % \n",
            " Completed: 75.00 % \n",
            " Completed: 80.00 % \n",
            " Completed: 85.00 % \n",
            " Completed: 90.00 % \n",
            " Completed: 95.00 % \n",
            " Completed: 100.00 % \n",
            " Loaded File: train_stacknet.csv\n",
            " Total rows in the file: 500\n",
            " Total columns in the file: 286\n",
            " Weighted variable : -1 counts: 0\n",
            " Int Id variable : -1 str id: -1 counts: 0\n",
            " Target Variables  : 1 values : [0]\n",
            " Actual columns number  : 285\n",
            " Number of Skipped rows   : 0\n",
            " Actual Rows (removing the skipped ones)  : 500\n",
            "Loaded dense train data with 500 and columns 285\n",
            " loaded data in : 0.161000\n",
            " Level: 1 dimensionality: 9\n",
            " Starting cross validation \n",
            "Fitting model: 1\n",
            "Fitting model: 2\n",
            "Fitting model: 3\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/gbce4a5sdcu8ltimju18p28una.conf']\n",
            "Train on 320 samples, validate on 80 samples\n",
            "Epoch 1/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 29s - loss: 1.2726\n",
            "144/320 [============>.................] - ETA: 0s - loss: 1.0722 \n",
            "256/320 [=======================>......] - ETA: 0s - loss: 1.0053\n",
            "320/320 [==============================] - 1s 3ms/step - loss: 1.0054 - val_loss: 0.8699\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.86986, saving model to /content/models/gbce4a5sdcu8ltimju18p28una.mod\n",
            "Epoch 2/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.6282\n",
            "144/320 [============>.................] - ETA: 0s - loss: 0.7410\n",
            "264/320 [=======================>......] - ETA: 0s - loss: 0.7102\n",
            "320/320 [==============================] - 0s 474us/step - loss: 0.7294 - val_loss: 0.8658\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.86986 to 0.86575, saving model to /content/models/gbce4a5sdcu8ltimju18p28una.mod\n",
            "Epoch 3/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3553\n",
            "112/320 [=========>....................] - ETA: 0s - loss: 0.5843\n",
            "192/320 [=================>............] - ETA: 0s - loss: 0.5957\n",
            "304/320 [===========================>..] - ETA: 0s - loss: 0.6384\n",
            "320/320 [==============================] - 0s 552us/step - loss: 0.6225 - val_loss: 0.8563\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86575 to 0.85625, saving model to /content/models/gbce4a5sdcu8ltimju18p28una.mod\n",
            "Epoch 4/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3433\n",
            "152/320 [=============>................] - ETA: 0s - loss: 0.6700\n",
            "280/320 [=========================>....] - ETA: 0s - loss: 0.7231\n",
            "320/320 [==============================] - 0s 460us/step - loss: 0.7327 - val_loss: 0.7038\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.85625 to 0.70377, saving model to /content/models/gbce4a5sdcu8ltimju18p28una.mod\n",
            "Epoch 5/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.7488\n",
            "136/320 [===========>..................] - ETA: 0s - loss: 0.5530\n",
            "232/320 [====================>.........] - ETA: 0s - loss: 0.6209\n",
            "320/320 [==============================] - 0s 479us/step - loss: 0.6028 - val_loss: 0.9048\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.70377\n",
            "Epoch 6/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.5771\n",
            "144/320 [============>.................] - ETA: 0s - loss: 0.5344\n",
            "264/320 [=======================>......] - ETA: 0s - loss: 0.5916\n",
            "320/320 [==============================] - 0s 480us/step - loss: 0.5609 - val_loss: 0.9234\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.70377\n",
            "Epoch 7/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.8450\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.5532\n",
            "256/320 [=======================>......] - ETA: 0s - loss: 0.5740\n",
            "320/320 [==============================] - 0s 459us/step - loss: 0.5715 - val_loss: 0.8873\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.70377\n",
            "Epoch 8/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4084\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.5443\n",
            "216/320 [===================>..........] - ETA: 0s - loss: 0.5841\n",
            "320/320 [==============================] - 0s 508us/step - loss: 0.5712 - val_loss: 0.9369\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.70377\n",
            "Epoch 9/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.2607\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.5713\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "256/320 [=======================>......] - ETA: 0s - loss: 0.6313\r\n",
            "320/320 [==============================] - 0s 445us/step - loss: 0.6181 - val_loss: 0.9655\r\n",
            "\r\n",
            "Epoch 00009: val_loss did not improve from 0.70377\r\n",
            "Epoch 10/20\r\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3581\n",
            "136/320 [===========>..................] - ETA: 0s - loss: 0.6262\n",
            "272/320 [========================>.....] - ETA: 0s - loss: 0.5694\n",
            "320/320 [==============================] - 0s 439us/step - loss: 0.5578 - val_loss: 1.0109\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.70377\n",
            "Epoch 11/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4302\n",
            "144/320 [============>.................] - ETA: 0s - loss: 0.5550\n",
            "264/320 [=======================>......] - ETA: 0s - loss: 0.5326\n",
            "320/320 [==============================] - 0s 448us/step - loss: 0.5352 - val_loss: 0.9108\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.70377\n",
            "Epoch 12/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4735\n",
            "136/320 [===========>..................] - ETA: 0s - loss: 0.4475\n",
            "264/320 [=======================>......] - ETA: 0s - loss: 0.5057\n",
            "320/320 [==============================] - 0s 464us/step - loss: 0.5175 - val_loss: 0.8149\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.70377\n",
            "Epoch 13/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4383\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.4755\n",
            "264/320 [=======================>......] - ETA: 0s - loss: 0.5121\n",
            "320/320 [==============================] - 0s 445us/step - loss: 0.4989 - val_loss: 0.8135\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.70377\n",
            "Epoch 14/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.9695\n",
            "144/320 [============>.................] - ETA: 0s - loss: 0.5365\n",
            "264/320 [=======================>......] - ETA: 0s - loss: 0.5297\n",
            "320/320 [==============================] - 0s 455us/step - loss: 0.5331 - val_loss: 0.9188\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "lib/python/KerasnnClassifier.py:283: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"lecun_uniform\")`\n",
            "  models.add(Dense(output_d, init=weight_in))\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.70377\n",
            "Epoch 00014: early stopping\n",
            " logloss : 0.7760798699519157\n",
            " logloss : 0.76718394997182\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/gbce4a5sdcu8ltimju18p28una.conf']\n",
            "\n",
            "  8/100 [=>............................] - ETA: 0s\n",
            "100/100 [==============================] - 0s 530us/step\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            " logloss : 0.9421592412372968\n",
            "Done with fold: 1/5\n",
            "Fitting model: 1\n",
            "Fitting model: 2\n",
            "Fitting model: 3\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/g3msf88lhakgh38vbhgt9liijg.conf']\n",
            "Train on 320 samples, validate on 80 samples\n",
            "Epoch 1/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 30s - loss: 1.4011\n",
            "136/320 [===========>..................] - ETA: 1s - loss: 1.1482 \n",
            "248/320 [======================>.......] - ETA: 0s - loss: 1.1001\n",
            "320/320 [==============================] - 1s 3ms/step - loss: 1.0380 - val_loss: 0.8405\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.84046, saving model to /content/models/g3msf88lhakgh38vbhgt9liijg.mod\n",
            "Epoch 2/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4196\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.6829\n",
            "248/320 [======================>.......] - ETA: 0s - loss: 0.7246\n",
            "320/320 [==============================] - 0s 461us/step - loss: 0.7371 - val_loss: 0.9204\n",
            "\n",
            "Epoch 00002: val_loss did not improve from 0.84046\n",
            "Epoch 3/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4046\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.6498\n",
            "208/320 [==================>...........] - ETA: 0s - loss: 0.6349\n",
            "320/320 [==============================] - 0s 544us/step - loss: 0.6433 - val_loss: 0.8407\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.84046\n",
            "Epoch 4/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3812\n",
            "136/320 [===========>..................] - ETA: 0s - loss: 0.6456\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "256/320 [=======================>......] - ETA: 0s - loss: 0.6841\r\n",
            "320/320 [==============================] - 0s 451us/step - loss: 0.7096 - val_loss: 0.8872\r\n",
            "\r\n",
            "Epoch 00004: val_loss did not improve from 0.84046\r\n",
            "Epoch 5/20\r\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.6675\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.5667\n",
            "256/320 [=======================>......] - ETA: 0s - loss: 0.6189\n",
            "320/320 [==============================] - 0s 473us/step - loss: 0.6081 - val_loss: 0.8776\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.84046\n",
            "Epoch 6/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3414\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.5951\n",
            "256/320 [=======================>......] - ETA: 0s - loss: 0.6548\n",
            "320/320 [==============================] - 0s 468us/step - loss: 0.6364 - val_loss: 1.0428\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.84046\n",
            "Epoch 7/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.6080\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.4701\n",
            "232/320 [====================>.........] - ETA: 0s - loss: 0.5372\n",
            "320/320 [==============================] - 0s 522us/step - loss: 0.5405 - val_loss: 1.0061\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.84046\n",
            "Epoch 8/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4588\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.5224\n",
            "240/320 [=====================>........] - ETA: 0s - loss: 0.5826\n",
            "320/320 [==============================] - 0s 493us/step - loss: 0.5570 - val_loss: 1.0345\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.84046\n",
            "Epoch 9/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.2966\n",
            "104/320 [========>.....................] - ETA: 0s - loss: 0.5257\n",
            "248/320 [======================>.......] - ETA: 0s - loss: 0.6170\n",
            "320/320 [==============================] - 0s 466us/step - loss: 0.6151 - val_loss: 1.1419\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.84046\n",
            "Epoch 10/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3609\n",
            "144/320 [============>.................] - ETA: 0s - loss: 0.5411\n",
            "280/320 [=========================>....] - ETA: 0s - loss: 0.5102\n",
            "320/320 [==============================] - 0s 478us/step - loss: 0.5001 - val_loss: 1.1960\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.84046\n",
            "Epoch 11/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.8859\n",
            "112/320 [=========>....................] - ETA: 0s - loss: 0.5816\n",
            "232/320 [====================>.........] - ETA: 0s - loss: 0.5377\n",
            "320/320 [==============================] - 0s 500us/step - loss: 0.5435 - val_loss: 1.1668\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "lib/python/KerasnnClassifier.py:283: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"lecun_uniform\")`\n",
            "  models.add(Dense(output_d, init=weight_in))\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.84046\n",
            "Epoch 00011: early stopping\n",
            " logloss : 0.729877923368898\n",
            " logloss : 0.7943920205762438\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/g3msf88lhakgh38vbhgt9liijg.conf']\n",
            "\n",
            "  8/100 [=>............................] - ETA: 0s\n",
            "100/100 [==============================] - 0s 567us/step\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            " logloss : 0.9029134908100319\n",
            "Done with fold: 2/5\n",
            "Fitting model: 1\n",
            "Fitting model: 2\n",
            "Fitting model: 3\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/qoj488otfcahi1oiscbajspkls.conf']\n",
            "Train on 320 samples, validate on 80 samples\n",
            "Epoch 1/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 30s - loss: 1.4072\n",
            " 96/320 [========>.....................] - ETA: 1s - loss: 1.2782 \n",
            "192/320 [=================>............] - ETA: 0s - loss: 1.2328\n",
            "296/320 [==========================>...] - ETA: 0s - loss: 1.1224\n",
            "320/320 [==============================] - 1s 3ms/step - loss: 1.1101 - val_loss: 0.9313\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.93131, saving model to /content/models/qoj488otfcahi1oiscbajspkls.mod\n",
            "Epoch 2/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.6098\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.6757\n",
            "216/320 [===================>..........] - ETA: 0s - loss: 0.6780\n",
            "320/320 [==============================] - 0s 522us/step - loss: 0.7365 - val_loss: 0.8480\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.93131 to 0.84801, saving model to /content/models/qoj488otfcahi1oiscbajspkls.mod\n",
            "Epoch 3/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.5044"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r\n",
            "144/320 [============>.................] - ETA: 0s - loss: 0.6530\n",
            "264/320 [=======================>......] - ETA: 0s - loss: 0.6756\n",
            "320/320 [==============================] - 0s 462us/step - loss: 0.6437 - val_loss: 0.7826\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.84801 to 0.78263, saving model to /content/models/qoj488otfcahi1oiscbajspkls.mod\n",
            "Epoch 4/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.1953\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.5910\n",
            "256/320 [=======================>......] - ETA: 0s - loss: 0.6561\n",
            "320/320 [==============================] - 0s 461us/step - loss: 0.6600 - val_loss: 0.8396\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.78263\n",
            "Epoch 5/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4588\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.5957\n",
            "240/320 [=====================>........] - ETA: 0s - loss: 0.6439\n",
            "320/320 [==============================] - 0s 496us/step - loss: 0.6342 - val_loss: 0.9280\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.78263\n",
            "Epoch 6/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3677\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.5380\n",
            "224/320 [====================>.........] - ETA: 0s - loss: 0.5462\n",
            "320/320 [==============================] - 0s 501us/step - loss: 0.5884 - val_loss: 0.9511\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.78263\n",
            "Epoch 7/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4085\n",
            "104/320 [========>.....................] - ETA: 0s - loss: 0.4927\n",
            "200/320 [=================>............] - ETA: 0s - loss: 0.5399\n",
            "288/320 [==========================>...] - ETA: 0s - loss: 0.5935\n",
            "320/320 [==============================] - 0s 589us/step - loss: 0.6133 - val_loss: 1.0438\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.78263\n",
            "Epoch 8/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4789\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.6054\n",
            "232/320 [====================>.........] - ETA: 0s - loss: 0.5710\n",
            "320/320 [==============================] - 0s 534us/step - loss: 0.6020 - val_loss: 1.0169\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.78263\n",
            "Epoch 9/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4044\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.6292\n",
            "264/320 [=======================>......] - ETA: 0s - loss: 0.6076\n",
            "320/320 [==============================] - 0s 460us/step - loss: 0.5921 - val_loss: 0.9871\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.78263\n",
            "Epoch 10/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.7875\n",
            "112/320 [=========>....................] - ETA: 0s - loss: 0.4547\n",
            "232/320 [====================>.........] - ETA: 0s - loss: 0.4742\n",
            "320/320 [==============================] - 0s 504us/step - loss: 0.4948 - val_loss: 1.0732\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.78263\n",
            "Epoch 11/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.7902\n",
            " 96/320 [========>.....................] - ETA: 0s - loss: 0.5867\n",
            "200/320 [=================>............] - ETA: 0s - loss: 0.5442\n",
            "304/320 [===========================>..] - ETA: 0s - loss: 0.5298\n",
            "320/320 [==============================] - 0s 581us/step - loss: 0.5291 - val_loss: 1.0658\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.78263\n",
            "Epoch 12/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.7049\n",
            "144/320 [============>.................] - ETA: 0s - loss: 0.5055\n",
            "248/320 [======================>.......] - ETA: 0s - loss: 0.5132\n",
            "320/320 [==============================] - 0s 501us/step - loss: 0.5120 - val_loss: 1.1262\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.78263\n",
            "Epoch 13/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3390\n",
            "104/320 [========>.....................] - ETA: 0s - loss: 0.4887\n",
            "224/320 [====================>.........] - ETA: 0s - loss: 0.5012\n",
            "320/320 [==============================] - 0s 529us/step - loss: 0.4901 - val_loss: 1.1883\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "lib/python/KerasnnClassifier.py:283: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"lecun_uniform\")`\n",
            "  models.add(Dense(output_d, init=weight_in))\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.78263\n",
            "Epoch 00013: early stopping\n",
            " logloss : 0.7084650027202057\n",
            " logloss : 0.6334118656943822\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/qoj488otfcahi1oiscbajspkls.conf']\n",
            "\n",
            "  8/100 [=>............................] - ETA: 0s\n",
            "100/100 [==============================] - 0s 600us/step\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            " logloss : 0.7176813847961919\n",
            "Done with fold: 3/5\n",
            "Fitting model: 1\n",
            "Fitting model: 2\n",
            "Fitting model: 3\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/hkt1aku3ktjsjohu2tdtvouvsa.conf']"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r\n",
            "Train on 320 samples, validate on 80 samples\r\n",
            "Epoch 1/20\r\n",
            "\r\n",
            "  8/320 [..............................] - ETA: 30s - loss: 1.4133\n",
            "104/320 [========>.....................] - ETA: 1s - loss: 1.2405 \n",
            "224/320 [====================>.........] - ETA: 0s - loss: 1.0864\n",
            "320/320 [==============================] - 1s 3ms/step - loss: 1.0054 - val_loss: 0.9245\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.92453, saving model to /content/models/hkt1aku3ktjsjohu2tdtvouvsa.mod\n",
            "Epoch 2/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.5830\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.5978\n",
            "224/320 [====================>.........] - ETA: 0s - loss: 0.6639\n",
            "320/320 [==============================] - 0s 543us/step - loss: 0.7316 - val_loss: 0.8548\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.92453 to 0.85483, saving model to /content/models/hkt1aku3ktjsjohu2tdtvouvsa.mod\n",
            "Epoch 3/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4418\n",
            "136/320 [===========>..................] - ETA: 0s - loss: 0.6155\n",
            "248/320 [======================>.......] - ETA: 0s - loss: 0.6381\n",
            "320/320 [==============================] - 0s 490us/step - loss: 0.6366 - val_loss: 0.9648\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.85483\n",
            "Epoch 4/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.2427\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.6122\n",
            "256/320 [=======================>......] - ETA: 0s - loss: 0.6386\n",
            "320/320 [==============================] - 0s 497us/step - loss: 0.6177 - val_loss: 0.8968\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.85483\n",
            "Epoch 5/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.2029\n",
            "112/320 [=========>....................] - ETA: 0s - loss: 0.4089\n",
            "224/320 [====================>.........] - ETA: 0s - loss: 0.6052\n",
            "320/320 [==============================] - 0s 533us/step - loss: 0.5844 - val_loss: 1.1518\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.85483\n",
            "Epoch 6/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3809\n",
            "112/320 [=========>....................] - ETA: 0s - loss: 0.4608\n",
            "232/320 [====================>.........] - ETA: 0s - loss: 0.5447\n",
            "320/320 [==============================] - 0s 520us/step - loss: 0.5466 - val_loss: 1.1398\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.85483\n",
            "Epoch 7/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.5575\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.4547\n",
            "256/320 [=======================>......] - ETA: 0s - loss: 0.4899\n",
            "320/320 [==============================] - 0s 485us/step - loss: 0.5334 - val_loss: 1.1633\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.85483\n",
            "Epoch 8/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.2744\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.4509\n",
            "224/320 [====================>.........] - ETA: 0s - loss: 0.4569\n",
            "312/320 [============================>.] - ETA: 0s - loss: 0.5033\n",
            "320/320 [==============================] - 0s 555us/step - loss: 0.4963 - val_loss: 1.1533\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.85483\n",
            "Epoch 9/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.9080\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.5564\n",
            "248/320 [======================>.......] - ETA: 0s - loss: 0.5125\n",
            "320/320 [==============================] - 0s 471us/step - loss: 0.5074 - val_loss: 1.0930\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.85483\n",
            "Epoch 10/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.6640\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.4472\n",
            "216/320 [===================>..........] - ETA: 0s - loss: 0.4459\n",
            "320/320 [==============================] - 0s 508us/step - loss: 0.4606 - val_loss: 1.2529\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.85483\n",
            "Epoch 11/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3044\n",
            "136/320 [===========>..................] - ETA: 0s - loss: 0.5562\n",
            "272/320 [========================>.....] - ETA: 0s - loss: 0.5047\n",
            "320/320 [==============================] - 0s 426us/step - loss: 0.5035 - val_loss: 1.2723\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.85483\n",
            "Epoch 12/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.6332\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.3934\n",
            "240/320 [=====================>........] - ETA: 0s - loss: 0.4028\n",
            "320/320 [==============================] - 0s 491us/step - loss: 0.4443 - val_loss: 1.2414\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "lib/python/KerasnnClassifier.py:283: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"lecun_uniform\")`\n",
            "  models.add(Dense(output_d, init=weight_in))\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.85483\n",
            "Epoch 00012: early stopping\n",
            " logloss : 0.7386716286590135\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " logloss : 0.7017277029421473\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/hkt1aku3ktjsjohu2tdtvouvsa.conf']\n",
            "\n",
            "  8/100 [=>............................] - ETA: 0s\n",
            "100/100 [==============================] - 0s 548us/step\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            " logloss : 1.007696476280081\n",
            "Done with fold: 4/5\n",
            "Fitting model: 1\n",
            "Fitting model: 2\n",
            "Fitting model: 3\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/f3mteo6dpgrhm6q2uduv5o3v8v.conf']\n",
            "Train on 320 samples, validate on 80 samples\n",
            "Epoch 1/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 31s - loss: 1.4493\n",
            "128/320 [===========>..................] - ETA: 1s - loss: 1.1738 \n",
            "240/320 [=====================>........] - ETA: 0s - loss: 1.0395\n",
            "320/320 [==============================] - 1s 3ms/step - loss: 0.9911 - val_loss: 1.1276\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.12758, saving model to /content/models/f3mteo6dpgrhm6q2uduv5o3v8v.mod\n",
            "Epoch 2/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3685\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.5531\n",
            "232/320 [====================>.........] - ETA: 0s - loss: 0.6515\n",
            "320/320 [==============================] - 0s 527us/step - loss: 0.6610 - val_loss: 0.9643\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.12758 to 0.96427, saving model to /content/models/f3mteo6dpgrhm6q2uduv5o3v8v.mod\n",
            "Epoch 3/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4632\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.5714\n",
            "240/320 [=====================>........] - ETA: 0s - loss: 0.6423\n",
            "320/320 [==============================] - 0s 485us/step - loss: 0.6382 - val_loss: 0.8630\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.96427 to 0.86305, saving model to /content/models/f3mteo6dpgrhm6q2uduv5o3v8v.mod\n",
            "Epoch 4/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.6835\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.6197\n",
            "224/320 [====================>.........] - ETA: 0s - loss: 0.6413\n",
            "320/320 [==============================] - 0s 507us/step - loss: 0.6256 - val_loss: 0.9155\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.86305\n",
            "Epoch 5/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.6115\n",
            "136/320 [===========>..................] - ETA: 0s - loss: 0.5101\n",
            "264/320 [=======================>......] - ETA: 0s - loss: 0.6425\n",
            "320/320 [==============================] - 0s 441us/step - loss: 0.6094 - val_loss: 1.4006\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.86305\n",
            "Epoch 6/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.2866\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.5783\n",
            "208/320 [==================>...........] - ETA: 0s - loss: 0.5423\n",
            "312/320 [============================>.] - ETA: 0s - loss: 0.6021\n",
            "320/320 [==============================] - 0s 559us/step - loss: 0.6031 - val_loss: 1.1570\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.86305\n",
            "Epoch 7/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.2128\n",
            "112/320 [=========>....................] - ETA: 0s - loss: 0.4720\n",
            "224/320 [====================>.........] - ETA: 0s - loss: 0.5151\n",
            "320/320 [==============================] - 0s 558us/step - loss: 0.5838 - val_loss: 0.8864\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.86305\n",
            "Epoch 8/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4601\n",
            "120/320 [==========>...................] - ETA: 0s - loss: 0.5171\n",
            "224/320 [====================>.........] - ETA: 0s - loss: 0.4881\n",
            "320/320 [==============================] - 0s 545us/step - loss: 0.5228 - val_loss: 0.9618\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.86305\n",
            "Epoch 9/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.4137\n",
            "112/320 [=========>....................] - ETA: 0s - loss: 0.4925\n",
            "200/320 [=================>............] - ETA: 0s - loss: 0.5112\n",
            "312/320 [============================>.] - ETA: 0s - loss: 0.5211\n",
            "320/320 [==============================] - 0s 589us/step - loss: 0.5116 - val_loss: 1.1115\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.86305\n",
            "Epoch 10/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.5861\n",
            "112/320 [=========>....................] - ETA: 0s - loss: 0.3933\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "216/320 [===================>..........] - ETA: 0s - loss: 0.4361\r\n",
            "320/320 [==============================] - 0s 540us/step - loss: 0.4813 - val_loss: 0.9738\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.86305\n",
            "Epoch 11/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.5481\n",
            "112/320 [=========>....................] - ETA: 0s - loss: 0.4232\n",
            "208/320 [==================>...........] - ETA: 0s - loss: 0.4541\n",
            "320/320 [==============================] - 0s 523us/step - loss: 0.4673 - val_loss: 0.9753\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.86305\n",
            "Epoch 12/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.3791\n",
            "128/320 [===========>..................] - ETA: 0s - loss: 0.4011\n",
            "248/320 [======================>.......] - ETA: 0s - loss: 0.4239\n",
            "320/320 [==============================] - 0s 498us/step - loss: 0.4489 - val_loss: 1.1297\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.86305\n",
            "Epoch 13/20\n",
            "\n",
            "  8/320 [..............................] - ETA: 0s - loss: 0.5848\n",
            "104/320 [========>.....................] - ETA: 0s - loss: 0.3478\n",
            "224/320 [====================>.........] - ETA: 0s - loss: 0.3907\n",
            "320/320 [==============================] - 0s 515us/step - loss: 0.4295 - val_loss: 1.0476\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "lib/python/KerasnnClassifier.py:283: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"lecun_uniform\")`\n",
            "  models.add(Dense(output_d, init=weight_in))\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.86305\n",
            "Epoch 00013: early stopping\n",
            " logloss : 0.7655681535238286\n",
            " logloss : 0.9200232379822043\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/f3mteo6dpgrhm6q2uduv5o3v8v.conf']\n",
            "\n",
            "  8/100 [=>............................] - ETA: 0s\n",
            "100/100 [==============================] - 0s 610us/step\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            " logloss : 1.104840441181103\n",
            "Done with fold: 5/5\n",
            " Average of all folds model 0 : 0.7437325156447724\n",
            " Average of all folds model 1 : 0.7633477554333595\n",
            " Average of all folds model 2 : 0.9350582068609409\n",
            " Level: 1 start output modelling \n",
            "Fitting model : 1\n",
            "Fitting model : 2\n",
            "Fitting model : 3\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/fbroi57jodskl1cisdsfu3cfp7.conf']\n",
            "Train on 400 samples, validate on 100 samples\n",
            "Epoch 1/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 38s - loss: 2.0023\n",
            "112/400 [=======>......................] - ETA: 2s - loss: 1.3798 \n",
            "216/400 [===============>..............] - ETA: 0s - loss: 1.2049\n",
            "344/400 [========================>.....] - ETA: 0s - loss: 1.0724\n",
            "400/400 [==============================] - 1s 3ms/step - loss: 1.0148 - val_loss: 1.1134\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 1.11337, saving model to /content/models/fbroi57jodskl1cisdsfu3cfp7.mod\n",
            "Epoch 2/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.4807\n",
            "136/400 [=========>....................] - ETA: 0s - loss: 0.6692\n",
            "264/400 [==================>...........] - ETA: 0s - loss: 0.7063\n",
            "384/400 [===========================>..] - ETA: 0s - loss: 0.7195\n",
            "400/400 [==============================] - 0s 469us/step - loss: 0.7339 - val_loss: 0.8371\n",
            "\n",
            "Epoch 00002: val_loss improved from 1.11337 to 0.83714, saving model to /content/models/fbroi57jodskl1cisdsfu3cfp7.mod\n",
            "Epoch 3/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.3449\n",
            "136/400 [=========>....................] - ETA: 0s - loss: 0.5834\n",
            "264/400 [==================>...........] - ETA: 0s - loss: 0.6452\n",
            "392/400 [============================>.] - ETA: 0s - loss: 0.6739\n",
            "400/400 [==============================] - 0s 475us/step - loss: 0.6680 - val_loss: 0.9642\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 0.83714\n",
            "Epoch 4/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.5192\n",
            " 88/400 [=====>........................] - ETA: 0s - loss: 0.6689\n",
            "200/400 [==============>...............] - ETA: 0s - loss: 0.6155\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "328/400 [=======================>......] - ETA: 0s - loss: 0.6516\r\n",
            "400/400 [==============================] - 0s 532us/step - loss: 0.6663 - val_loss: 0.9641\r\n",
            "\r\n",
            "Epoch 00004: val_loss did not improve from 0.83714\r\n",
            "Epoch 5/20\r\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.7723\n",
            "136/400 [=========>....................] - ETA: 0s - loss: 0.6095\n",
            "264/400 [==================>...........] - ETA: 0s - loss: 0.6096\n",
            "360/400 [==========================>...] - ETA: 0s - loss: 0.6408\n",
            "400/400 [==============================] - 0s 496us/step - loss: 0.6496 - val_loss: 0.9842\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.83714\n",
            "Epoch 6/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.6524\n",
            "128/400 [========>.....................] - ETA: 0s - loss: 0.5499\n",
            "264/400 [==================>...........] - ETA: 0s - loss: 0.6374\n",
            "384/400 [===========================>..] - ETA: 0s - loss: 0.6307\n",
            "400/400 [==============================] - 0s 495us/step - loss: 0.6217 - val_loss: 0.8843\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.83714\n",
            "Epoch 7/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.5396\n",
            "112/400 [=======>......................] - ETA: 0s - loss: 0.6004\n",
            "216/400 [===============>..............] - ETA: 0s - loss: 0.6101\n",
            "328/400 [=======================>......] - ETA: 0s - loss: 0.6263\n",
            "400/400 [==============================] - 0s 541us/step - loss: 0.6230 - val_loss: 0.8233\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.83714 to 0.82332, saving model to /content/models/fbroi57jodskl1cisdsfu3cfp7.mod\n",
            "Epoch 8/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.5710\n",
            "120/400 [========>.....................] - ETA: 0s - loss: 0.4803\n",
            "232/400 [================>.............] - ETA: 0s - loss: 0.5834\n",
            "344/400 [========================>.....] - ETA: 0s - loss: 0.5817\n",
            "400/400 [==============================] - 0s 516us/step - loss: 0.5830 - val_loss: 0.8649\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.82332\n",
            "Epoch 9/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.3883\n",
            "120/400 [========>.....................] - ETA: 0s - loss: 0.5585\n",
            "232/400 [================>.............] - ETA: 0s - loss: 0.5511\n",
            "312/400 [======================>.......] - ETA: 0s - loss: 0.5739\n",
            "400/400 [==============================] - 0s 542us/step - loss: 0.5736 - val_loss: 0.9259\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.82332\n",
            "Epoch 10/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.5415\n",
            "136/400 [=========>....................] - ETA: 0s - loss: 0.4717\n",
            "272/400 [===================>..........] - ETA: 0s - loss: 0.5307\n",
            "400/400 [==============================] - 0s 450us/step - loss: 0.5583 - val_loss: 1.0542\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.82332\n",
            "Epoch 11/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.4355\n",
            "112/400 [=======>......................] - ETA: 0s - loss: 0.4363\n",
            "224/400 [===============>..............] - ETA: 0s - loss: 0.4385\n",
            "336/400 [========================>.....] - ETA: 0s - loss: 0.4520\n",
            "400/400 [==============================] - 0s 540us/step - loss: 0.4907 - val_loss: 0.9213\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.82332\n",
            "Epoch 12/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.6841\n",
            "120/400 [========>.....................] - ETA: 0s - loss: 0.4879\n",
            "200/400 [==============>...............] - ETA: 0s - loss: 0.4718\n",
            "312/400 [======================>.......] - ETA: 0s - loss: 0.4991\n",
            "400/400 [==============================] - 0s 547us/step - loss: 0.5230 - val_loss: 1.0018\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.82332\n",
            "Epoch 13/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 1.0688\n",
            "120/400 [========>.....................] - ETA: 0s - loss: 0.4941\n",
            "216/400 [===============>..............] - ETA: 0s - loss: 0.4954\n",
            "296/400 [=====================>........] - ETA: 0s - loss: 0.5275\n",
            "400/400 [==============================] - 0s 586us/step - loss: 0.5255 - val_loss: 1.0278\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.82332\n",
            "Epoch 14/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.4049\n",
            "104/400 [======>.......................] - ETA: 0s - loss: 0.5640\n",
            "184/400 [============>.................] - ETA: 0s - loss: 0.5218\n",
            "296/400 [=====================>........] - ETA: 0s - loss: 0.5307\n",
            "400/400 [==============================] - 0s 590us/step - loss: 0.5183 - val_loss: 1.0461\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.82332\n",
            "Epoch 15/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.2231\n",
            "120/400 [========>.....................] - ETA: 0s - loss: 0.4383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "232/400 [================>.............] - ETA: 0s - loss: 0.4514\n",
            "344/400 [========================>.....] - ETA: 0s - loss: 0.4778\n",
            "400/400 [==============================] - 0s 514us/step - loss: 0.4939 - val_loss: 1.2012\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.82332\n",
            "Epoch 16/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.5576\n",
            "120/400 [========>.....................] - ETA: 0s - loss: 0.4379\n",
            "224/400 [===============>..............] - ETA: 0s - loss: 0.5054\n",
            "328/400 [=======================>......] - ETA: 0s - loss: 0.4837\n",
            "400/400 [==============================] - 0s 527us/step - loss: 0.4890 - val_loss: 0.9308\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.82332\n",
            "Epoch 17/20\n",
            "\n",
            "  8/400 [..............................] - ETA: 0s - loss: 0.2111\n",
            "128/400 [========>.....................] - ETA: 0s - loss: 0.4183\n",
            "248/400 [=================>............] - ETA: 0s - loss: 0.4947\n",
            "376/400 [===========================>..] - ETA: 0s - loss: 0.5140\n",
            "400/400 [==============================] - 0s 459us/step - loss: 0.5147 - val_loss: 1.0722\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            "lib/python/KerasnnClassifier.py:283: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(3, kernel_initializer=\"lecun_uniform\")`\n",
            "  models.add(Dense(output_d, init=weight_in))\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.82332\n",
            "Epoch 00017: early stopping\n",
            "Completed level: 1 out of 2\n",
            " Level: 2 dimensionality: 3\n",
            " Starting cross validation \n",
            " Average of all folds model 0 : 0.0\n",
            " Level: 2 start output modelling \n",
            "Fitting model : 1\n",
            "Completed level: 2 out of 2\n",
            " modelling lasted : 113.761000\n",
            " Completed: 5.00 % \n",
            " Completed: 10.00 % \n",
            " Completed: 15.00 % \n",
            " Completed: 20.00 % \n",
            " Completed: 25.00 % \n",
            " Completed: 30.00 % \n",
            " Completed: 35.00 % \n",
            " Completed: 40.00 % \n",
            " Completed: 45.00 % \n",
            " Completed: 50.00 % \n",
            " Completed: 55.00 % \n",
            " Completed: 60.00 % \n",
            " Completed: 65.00 % \n",
            " Completed: 70.00 % \n",
            " Completed: 75.00 % \n",
            " Completed: 80.00 % \n",
            " Completed: 85.00 % \n",
            " Completed: 90.00 % \n",
            " Completed: 95.00 % \n",
            " Completed: 100.00 % \n",
            " Loaded File: test_stacknet.csv\n",
            " Total rows in the file: 500\n",
            " Total columns in the file: 286\n",
            " Weighted variable : -1 counts: 0\n",
            " Int Id variable : -1 str id: -1 counts: 0\n",
            " Target Variables  : 1 values : [0]\n",
            " Actual columns number  : 285\n",
            " Number of Skipped rows   : 0\n",
            " Actual Rows (removing the skipped ones)  : 500\n",
            "Loaded dense test data with 500 and columns 285\n",
            " loading test data lasted : 0.057000\n",
            "arguments:  ['lib/python/KerasnnClassifier.py', '/content/models/fbroi57jodskl1cisdsfu3cfp7.conf']\n",
            "\n",
            "  8/500 [..............................] - ETA: 3s\n",
            "456/500 [==========================>...] - ETA: 0s\n",
            "500/500 [==============================] - 0s 216us/step\n",
            "/usr/local/lib/python3.6/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
            "  from ._conv import register_converters as _register_converters\n",
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
            "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
            " Completed: 5.00 % \n",
            " Completed: 10.00 % \n",
            " Completed: 15.00 % \n",
            " Completed: 20.00 % \n",
            " Completed: 25.00 % \n",
            " Completed: 30.00 % \n",
            " Completed: 35.00 % \n",
            " Completed: 40.00 % \n",
            " Completed: 45.00 % \n",
            " Completed: 50.00 % \n",
            " Completed: 55.00 % \n",
            " Completed: 60.00 % \n",
            " Completed: 65.00 % \n",
            " Completed: 70.00 % \n",
            " Completed: 75.00 % \n",
            " Completed: 80.00 % \n",
            " Completed: 85.00 % \n",
            " Completed: 90.00 % \n",
            " Completed: 95.00 % \n",
            " Completed: 100.00 % \n",
            " predicting on test data lasted : 4.444000\n",
            "Metric could not be calculated on the test \n",
            " The whole StackNet procedure lasted: 119.267000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "arMPxnO0yUOw",
        "colab_type": "code",
        "colab": {
          "autoexec": {
            "startup": false,
            "wait_interval": 0
          },
          "base_uri": "https://localhost:8080/",
          "height": 9455
        },
        "outputId": "63f0903a-d32d-4128-941a-a91600c1dbdf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1525062129346,
          "user_tz": -540,
          "elapsed": 1941,
          "user": {
            "displayName": "John Hwang",
            "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s128",
            "userId": "100225940964623691936"
          }
        }
      },
      "cell_type": "code",
      "source": [
        "!cat lib/python/KerasnnClassifier.py"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# -*- coding: utf-8 -*-\r\n",
            "\"\"\"\r\n",
            "Copyright (c) 2017 Marios Michailidis\r\n",
            "\r\n",
            "Permission is hereby granted, free of charge, to any person obtaining a copy\r\n",
            "of this software and associated documentation files (the \"Software\"), to deal\r\n",
            "in the Software without restriction, including without limitation the rights\r\n",
            "to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\r\n",
            "copies of the Software, and to permit persons to whom the Software is\r\n",
            "furnished to do so, subject to the following conditions:\r\n",
            "\r\n",
            "The above copyright notice and this permission notice shall be included in all\r\n",
            "copies or substantial portions of the Software.\r\n",
            "\r\n",
            "THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n",
            "IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n",
            "FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\r\n",
            "AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n",
            "LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\r\n",
            "OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\r\n",
            "SOFTWARE.\r\n",
            "\r\n",
            "Created on Wed Aug 02 16:49:28 2017\r\n",
            "\r\n",
            "@author: Marios Michailidis\r\n",
            "\r\n",
            "\r\n",
            "Supplementary python script to perform keras's ANN, to be run in conjuction with \r\n",
            "StackNet's KerasnnClassifier.java class\r\n",
            "\r\n",
            "Parameters:\r\n",
            "    \r\n",
            "l2=0.0,0.0\r\n",
            "momentum=0.9\r\n",
            "epochs=10\r\n",
            "batch_size=64\r\n",
            "stopping_rounds=0\r\n",
            "threads=1\r\n",
            "use_dense=True\t\t\r\n",
            "validation_split=0.0\r\n",
            "copy=false\r\n",
            "seed=1\r\n",
            "lr=0.01\t\t\r\n",
            "shuffle=true\r\n",
            "standardize=true\t\t\t\r\n",
            "batch_normalization=true\t\t\r\n",
            "use_log1p=false\t\t\t\r\n",
            "hidden=50,25\t\t\r\n",
            "droupouts=0.4,02\t\r\n",
            "weight_init=lecun_uniform\t\t\r\n",
            "activation=relu,relu\r\n",
            "optimizer=adam\r\n",
            "loss=categorical_crossentropy\r\n",
            "verbose=0\t\r\n",
            "\"\"\"\r\n",
            "\r\n",
            "import sys\r\n",
            "import os\r\n",
            "from sklearn.externals import joblib\r\n",
            "from sklearn.datasets import load_svmlight_file\r\n",
            "import numpy as np\r\n",
            "from sklearn.preprocessing import StandardScaler,MaxAbsScaler\r\n",
            "from scipy.sparse import csr_matrix \r\n",
            "import keras\r\n",
            "from keras.optimizers import Adam,Adagrad,SGD,Nadam,Adadelta\r\n",
            "from keras.layers.normalization import BatchNormalization\r\n",
            "from keras.models import Sequential\r\n",
            "from keras.layers.core import Dense, Dropout, Activation\r\n",
            "from keras.utils import np_utils\r\n",
            "from keras import regularizers\r\n",
            "from keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
            "import h5py\r\n",
            "import gc\r\n",
            "from sklearn import cross_validation\r\n",
            "\"\"\"\r\n",
            "conf_name: name with parameters\r\n",
            "params : python dictionary with parameters and types\r\n",
            "return: several filenames and a new parameters's file with the actual values\r\n",
            "\"\"\"\r\n",
            "\r\n",
            "def read_file_end_return_parameters(conf_name, params):\r\n",
            "    new_params={}\r\n",
            "    Use_dense=False\r\n",
            "    columns=0\r\n",
            "    task=\"\"\r\n",
            "    model_name=\"\"\r\n",
            "    data_name=\"\"\r\n",
            "    prediction_name=\"\"\r\n",
            "\r\n",
            "    if not os.path.isfile(conf_name):\r\n",
            "        raise Exception(\" %s config file does not exist... \" % (conf_name)) \r\n",
            "\r\n",
            "    f_file=open(conf_name, \"r\")\r\n",
            "    for line in  f_file:\r\n",
            "        line=line.replace(\"\\n\",\"\").replace(\"\\r\",\"\")\r\n",
            "        splits=line.split(\"=\")\r\n",
            "        if len(splits)!=2:\r\n",
            "            raise Exception(\" this (%s) line in %s config file has the wrong format..the corerct format should be: parameter=value \" % (line,conf_name))             \r\n",
            "        parameter=splits[0]\r\n",
            "        value=splits[1]\r\n",
            "        if parameter==\"task\":\r\n",
            "            task=value\r\n",
            "        elif parameter==\"usedense\":\r\n",
            "            if value.lower()==\"true\":\r\n",
            "                Use_dense=True\r\n",
            "            else :\r\n",
            "                Use_dense=False   \r\n",
            "        elif parameter==\"columns\": \r\n",
            "            try:\r\n",
            "                columns=int(value)  \r\n",
            "            except:\r\n",
            "                raise Exception(\" Parameter %s is expecting a int value but the current could nto be converted: %s \" % (parameter,value))                                       \r\n",
            "        \r\n",
            "        elif parameter==\"model\": \r\n",
            "            model_name=value               \r\n",
            "        elif parameter==\"data\": \r\n",
            "            data_name=value  \r\n",
            "        elif parameter==\"prediction\": \r\n",
            "            prediction_name=value  \r\n",
            "        else : # it must be a model parameter\r\n",
            "            #search if parameter is in the file\r\n",
            "            if parameter not in  params:\r\n",
            "                raise Exception(\" Parameter %s is not recognised \" % (parameter))     \r\n",
            "            else :\r\n",
            "                paramaeter_type=params[parameter]\r\n",
            "                if paramaeter_type==\"bool\":\r\n",
            "                    if value.lower()==\"true\":\r\n",
            "                        new_params[parameter]=True\r\n",
            "                    else :\r\n",
            "                        new_params[parameter]=False\r\n",
            "                elif paramaeter_type==\"str\":\r\n",
            "                    new_params[parameter]=value                                  \r\n",
            "                elif paramaeter_type==\"float\":\r\n",
            "                    try:\r\n",
            "                        new_params[parameter]=float(value)\r\n",
            "                    except:\r\n",
            "                        raise Exception(\" Parameter %s is expecting a float value but the current could nto be converted: %s \" % (parameter,value))                                       \r\n",
            "                elif paramaeter_type==\"int\":\r\n",
            "                    try:\r\n",
            "                        new_params[parameter]=int(value)\r\n",
            "                    except:\r\n",
            "                        raise Exception(\" Parameter %s is expecting a int value but the current could nto be converted: %s \" % (parameter,value))                                       \r\n",
            "\r\n",
            "                elif paramaeter_type==\"float_list\":\r\n",
            "                    splits=value.replace(\" \",\"\").split(\",\")\r\n",
            "                    float_values=[]\r\n",
            "                    for values in splits: \r\n",
            "                        try:\r\n",
            "                           float_values.append(float(values))\r\n",
            "                        except:\r\n",
            "                            raise Exception(\" Parameter %s is expecting a comma separated string of floats but the current string count not be converted to a list: %s \" % (parameter,value))                                       \r\n",
            "                    \r\n",
            "                    new_params[parameter]=float_values\r\n",
            "                elif paramaeter_type==\"int_list\":\r\n",
            "                    splits=value.replace(\" \",\"\").split(\",\")\r\n",
            "                    int_values=[]\r\n",
            "                    for values in splits: \r\n",
            "                        try:\r\n",
            "                           int_values.append(int(values))\r\n",
            "                        except:\r\n",
            "                            raise Exception(\" Parameter %s is expecting a comma separated string of ints but the current string count not be converted to a list: %s \" % (parameter,value))                                       \r\n",
            "                    \r\n",
            "                    new_params[parameter]=int_values                    \r\n",
            "                elif paramaeter_type==\"str_list\":\r\n",
            "                    splits=value.replace(\" \",\"\").split(\",\")\r\n",
            "                    str_values=[]\r\n",
            "                    for values in splits: \r\n",
            "                           str_values.append(values)\r\n",
            "                    new_params[parameter]=str_values                      \r\n",
            "                     \r\n",
            "                else :\r\n",
            "                    raise Exception(\" Parameter type %s is not recognised \" % (paramaeter_type)) \r\n",
            "    f_file.close()\r\n",
            "                \r\n",
            "    return  Use_dense,task,model_name,data_name,prediction_name,columns, new_params               \r\n",
            "\r\n",
            "\r\n",
            "\"\"\"\r\n",
            "Creates a keras model with 'sequential' format\r\n",
            "input_dum: column dimension of training data\r\n",
            "output_dum: output of the last layer\r\n",
            "params : model parameters\r\n",
            "returns : keras sequential model\r\n",
            "\"\"\" \r\n",
            "\r\n",
            "def build_model(input_d, output_d, params):\r\n",
            "    \r\n",
            "    hidden=[]\r\n",
            "    if \"hidden\" not in params :\r\n",
            "        raise Exception(\"hidden is a mandatory parameter\")\r\n",
            "    hidden=params[\"hidden\"]\r\n",
            "    dropouts=[]\r\n",
            "    if \"droupouts\"  in params :\r\n",
            "        dropouts=   params[\"droupouts\"]\r\n",
            "    else :\r\n",
            "        dropouts=[0.0001 for s in range(0,len(hidden)) ]\r\n",
            "    if len(dropouts)<len(hidden):\r\n",
            "        while len(dropouts)!=len(hidden):\r\n",
            "            dropouts.append(0.0001)\r\n",
            "    if len(dropouts)>len(hidden):\r\n",
            "        while len(dropouts)!=len(hidden):\r\n",
            "            dropouts.pop()   \r\n",
            "\r\n",
            "    activations=[]\r\n",
            "    if \"activation\"  in params :\r\n",
            "        activations=   params[\"activation\"]\r\n",
            "    else :\r\n",
            "        activations=[\"relu\" for s in range(0,len(hidden)) ]\r\n",
            "    if len(activations)<len(hidden):\r\n",
            "        while len(activations)!=len(hidden):\r\n",
            "            activations.append(\"relu\")\r\n",
            "    if len(activations)>len(hidden):\r\n",
            "        while len(activations)!=len(hidden):\r\n",
            "            activations.pop()   \r\n",
            "            \r\n",
            "    l2s=[]\r\n",
            "    if \"l2\"  in params :\r\n",
            "        l2s=   params[\"l2\"]\r\n",
            "    else :\r\n",
            "        l2s=[0.0 for s in range(0,len(hidden)) ]\r\n",
            "    if len(l2s)<len(hidden):\r\n",
            "        while len(l2s)!=len(hidden):\r\n",
            "            l2s.append(0.0)\r\n",
            "    if len(l2s)>len(hidden):\r\n",
            "        while len(l2s)!=len(hidden):\r\n",
            "            l2s.pop()   \r\n",
            "            \r\n",
            "    weight_in='lecun_uniform'\r\n",
            "    if \"weight_init\" in params :\r\n",
            "        weight_in= params[\"weight_init\"]\r\n",
            "        \r\n",
            "    loss= 'categorical_crossentropy'\r\n",
            "    if \"loss\" in params :\r\n",
            "        loss= params[\"loss\"]  \r\n",
            "        \r\n",
            "    lr=0.01\r\n",
            "    if \"lr\" in params :\r\n",
            "       lr = params[\"lr\"]\r\n",
            "       \r\n",
            "    optimzer= 'Adam'\r\n",
            "    if \"optimzer\" in params :\r\n",
            "        optimzer= params[\"optimzer\"] \r\n",
            "\r\n",
            "    momentum=0.01\r\n",
            "    if \"momentum\" in params :\r\n",
            "       momentum = params[\"momentum\"]\r\n",
            "       \r\n",
            "    use_batch=False   \r\n",
            "    if \"batch_normalization\" in params :\r\n",
            "       use_batch = params[\"batch_normalization\"]\r\n",
            "       \r\n",
            "       \r\n",
            "       \r\n",
            "    opt= Adam (lr=lr) \r\n",
            "    if optimzer.lower()==\"adagrad\":\r\n",
            "        opt= Adagrad (lr=lr)         \r\n",
            "    elif optimzer.lower()==\"nadam\":\r\n",
            "        opt= Nadam (lr=lr)   \r\n",
            "    elif optimzer.lower()==\"adadelta\":\r\n",
            "        opt= Adadelta (lr=lr)   \r\n",
            "    elif optimzer.lower()==\"sgd\":\r\n",
            "        opt= SGD (lr=lr, momentum=momentum, nesterov=True) \r\n",
            "        \r\n",
            "    models = Sequential()\r\n",
            "    \r\n",
            "    for h in range (len(hidden)):\r\n",
            "        unit=hidden[h]\r\n",
            "        dropout=dropouts[h]\r\n",
            "        l_2=l2s[h]\r\n",
            "        active=activations[h]\r\n",
            "        if h==0:\r\n",
            "            models.add(Dense(input_dim=input_d, units=unit, kernel_initializer=weight_in,kernel_regularizer=regularizers.l2(l_2) ))\r\n",
            "        else :\r\n",
            "            models.add(Dense( units=unit, kernel_initializer=weight_in,kernel_regularizer=regularizers.l2(l_2) ))            \r\n",
            "    \r\n",
            "        models.add(Activation(active))\r\n",
            "        if use_batch:\r\n",
            "            models.add(BatchNormalization())  \r\n",
            "        \r\n",
            "        models.add(Dropout(dropout))\r\n",
            "        \r\n",
            "\r\n",
            "    models.add(Dense(output_d, init=weight_in))\r\n",
            "    models.add(Activation('softmax'))\r\n",
            "\r\n",
            "    models.compile(loss=loss, optimizer=opt)\r\n",
            "    return models    \r\n",
            "\r\n",
            "\r\n",
            "def batch_generator(X, y, batch_size, shuffle):\r\n",
            "    number_of_batches = np.ceil(X.shape[0]/batch_size)\r\n",
            "    counter = 0\r\n",
            "    sample_index = np.arange(X.shape[0])\r\n",
            "    if shuffle:\r\n",
            "        np.random.shuffle(sample_index)\r\n",
            "    while True:\r\n",
            "        batch_index = sample_index[batch_size*counter:batch_size*(counter+1)]\r\n",
            "        X_batch = X[batch_index,:].toarray()\r\n",
            "        y_batch = y[batch_index]\r\n",
            "        counter += 1\r\n",
            "        yield X_batch, y_batch\r\n",
            "        if (counter == number_of_batches):\r\n",
            "            if shuffle:\r\n",
            "                np.random.shuffle(sample_index)\r\n",
            "            counter = 0\r\n",
            "\r\n",
            "def batch_generatorp(X, batch_size, shuffle):\r\n",
            "    number_of_batches = X.shape[0] / np.ceil(X.shape[0]/batch_size)\r\n",
            "    counter = 0\r\n",
            "    sample_index = np.arange(X.shape[0])\r\n",
            "    while True:\r\n",
            "        batch_index = sample_index[batch_size * counter:batch_size * (counter + 1)]\r\n",
            "        X_batch = X[batch_index, :].toarray()\r\n",
            "        counter += 1\r\n",
            "        yield X_batch\r\n",
            "        if (counter == number_of_batches):\r\n",
            "            counter = 0\r\n",
            "\r\n",
            "\r\n",
            "\r\n",
            "\"\"\"\r\n",
            "Loads svmlight data\r\n",
            "fname: filename to load\r\n",
            "returns X, y\r\n",
            "\"\"\"                            \r\n",
            "\r\n",
            "def get_data(fname,cols):\r\n",
            "    data = load_svmlight_file(fname,n_features =cols)\r\n",
            "    return data[0], data[1]\r\n",
            "            \r\n",
            "#main method to get executed when calling python\r\n",
            "def main():\r\n",
            "    \r\n",
            "\r\n",
            "    config_file=\"\"   \r\n",
            "    acceptable_parameters={\"standardize\" : \"bool\" ,\r\n",
            "                           \"use_log1p\" : \"bool\" ,\r\n",
            "                           \"shuffle\" : \"bool\" ,    \r\n",
            "                           \"batch_normalization\" : \"bool\" ,                            \r\n",
            "                           \"weight_init\" : \"str\" ,\r\n",
            "                           \"activation\" : \"str_list\" ,                           \r\n",
            "                           \"optimizer\" : \"str\" ,    \r\n",
            "                           \"loss\" : \"str\" ,                            \r\n",
            "                           \"l2\" : \"float_list\" ,\r\n",
            "                           \"hidden\" : \"int_list\" ,    \r\n",
            "                           \"droupouts\" : \"float_list\" ,                            \r\n",
            "                           \"momentum\" : \"float\" ,                           \r\n",
            "                           \"epochs\" : \"int\" ,  \r\n",
            "                           \"lr\" : \"float\" ,     \r\n",
            "                           \"batch_size\" : \"int\" ,\r\n",
            "                           \"stopping_rounds\" : \"int\" ,  \r\n",
            "                           \"validation_split\" : \"float\" ,                                       \r\n",
            "                           \"seed\" : \"int\" ,   \r\n",
            "                           \"verbose\" : \"int\",    \r\n",
            "                           \"threads\" : \"int\"                            \r\n",
            "                           }\r\n",
            "    \r\n",
            "\r\n",
            "\t\t\t    \r\n",
            "    \r\n",
            "    arguments=sys.argv\r\n",
            "    print (\"arguments: \",arguments )\r\n",
            "    if len(arguments)!=2:\r\n",
            "        raise Exception(\" was expecting only one argument pointing to the config file... process will terminate\")\r\n",
            "\r\n",
            "    else :\r\n",
            "        config_file=arguments[1] \r\n",
            "        dense,task_type,model_file,data_file,prediction_file,column, model_parameters=read_file_end_return_parameters(config_file, acceptable_parameters)   \r\n",
            "        #sanity checks\r\n",
            "        if task_type not in [\"train\",\"predict\"]:\r\n",
            "            raise Exception(\"task needs to be either train or predict, here it was %s ... \" % (task_type))   \r\n",
            "        if model_file==\"\":\r\n",
            "            raise Exception(\"model file cannot be empty\")       \r\n",
            "        if data_file==\"\":\r\n",
            "            raise Exception(\"data file file cannot be empty\")    \r\n",
            "        if not os.path.isfile(data_file):\r\n",
            "            raise Exception(\" %s data file does not exist... \" % (data_file))           \r\n",
            "        if task_type==\"predict\" and  prediction_file==\"\":\r\n",
            "            raise Exception(\"prediction file  cannot be empty when task=predict\")  \r\n",
            "        if len(model_parameters)==0:\r\n",
            "            raise Exception(\"model parameters cannot be empty\") \r\n",
            "        if column<1:\r\n",
            "            raise Exception(\"columns cannot be less than 1...\")             \r\n",
            "\r\n",
            "        batch_size=32\r\n",
            "        if  \"batch_size\" in model_parameters:\r\n",
            "            batch_size=model_parameters[\"batch_size\"]                \r\n",
            "        verbose=0\r\n",
            "        if  \"verbose\" in model_parameters:\r\n",
            "            verbose=model_parameters[\"verbose\"] \r\n",
            "            \r\n",
            "        ################### Model training ###############\r\n",
            "        if  task_type ==\"train\":\r\n",
            "            X,y=get_data(data_file, column) #load data\r\n",
            "            st=StandardScaler() \r\n",
            "            ab=MaxAbsScaler()\r\n",
            "            unique_labels=len(np.unique(y))\r\n",
            "           \r\n",
            "            shuffle=True\r\n",
            "            if  \"shuffle\" in model_parameters:\r\n",
            "                shuffle=model_parameters[\"shuffle\"]\r\n",
            "            epochs=1\r\n",
            "            if  \"epochs\" in model_parameters:\r\n",
            "                epochs=model_parameters[\"epochs\"]    \r\n",
            "            stopping_rounds=0\r\n",
            "            if  \"stopping_rounds\" in model_parameters:\r\n",
            "                stopping_rounds=model_parameters[\"stopping_rounds\"]\r\n",
            "            validation_split=0\r\n",
            "            if  \"validation_split\" in model_parameters:\r\n",
            "                validation_split=model_parameters[\"validation_split\"]    \r\n",
            "            seed=1\r\n",
            "            if  \"seed\" in model_parameters:\r\n",
            "                seed=model_parameters[\"seed\"]                    \r\n",
            "                \r\n",
            "            np.random.seed(seed)             \r\n",
            "            if dense: #convert to dense - useful if the data does nto have high dimensionality .\r\n",
            "\r\n",
            "               X=X.toarray()\r\n",
            "               if \"use_log1p\" in model_parameters and model_parameters[\"use_log1p\"]==True:\r\n",
            "                   X[X<0]=0\r\n",
            "                   X=np.log1p(X)                  \r\n",
            "               if \"standardize\" in model_parameters and model_parameters[\"standardize\"]==True:\r\n",
            "                   X=st.fit_transform(X)\r\n",
            "                   \r\n",
            "               model=build_model(X.shape[1],unique_labels,model_parameters )     \r\n",
            "               \r\n",
            "               if validation_split<=0.0:\r\n",
            "                                      \r\n",
            "                     model.fit( X, \r\n",
            "                                np_utils.to_categorical(y), \r\n",
            "                                epochs=epochs,\r\n",
            "                                verbose=verbose,\r\n",
            "                                batch_size=batch_size,\r\n",
            "                                shuffle=shuffle)\r\n",
            "\r\n",
            "               else :\r\n",
            "                  \r\n",
            "                   x_train_oof, x_valid_oof, y_train_oof_nn, y_valid_oof_nn = cross_validation.train_test_split(\r\n",
            "                            X, y, test_size=validation_split, random_state=seed) \r\n",
            "\r\n",
            "                   callbacks = [\r\n",
            "                    EarlyStopping(\r\n",
            "                        monitor='val_loss', \r\n",
            "                        patience=stopping_rounds,\r\n",
            "                        verbose=verbose,\r\n",
            "                         mode='auto'),\r\n",
            "                    ModelCheckpoint(\r\n",
            "                        model_file, \r\n",
            "                        monitor='val_loss', \r\n",
            "                        save_best_only=True, \r\n",
            "                        verbose=verbose)\r\n",
            "                    ]\r\n",
            "                    \r\n",
            "                   model.fit(\r\n",
            "                        x_train_oof, \r\n",
            "                        np_utils.to_categorical(y_train_oof_nn), \r\n",
            "                        epochs=epochs,\r\n",
            "                        validation_data=(x_valid_oof, np_utils.to_categorical(y_valid_oof_nn)),\r\n",
            "                        verbose=verbose,\r\n",
            "                        batch_size=batch_size,\r\n",
            "                        callbacks=callbacks,\r\n",
            "                        shuffle=shuffle)  \r\n",
            "                   \r\n",
            "               joblib.dump((st) , model_file+\".scaler\")                   \r\n",
            "               keras.models.save_model(model,model_file)\r\n",
            "               model=None\r\n",
            "               gc.collect()                     \r\n",
            "               if not os.path.isfile(model_file):\r\n",
            "                    raise Exception(\" %s model file could not be exported - check permissions ... \" % (model_file))\r\n",
            "        \r\n",
            "            else :\r\n",
            "               \r\n",
            "               if \"use_log1p\" in model_parameters and model_parameters[\"use_log1p\"]==True:\r\n",
            "                   X[X<0]=0\r\n",
            "                   X=csr_matrix(X).log1p()               \r\n",
            "               if \"standardize\" in model_parameters and model_parameters[\"standardize\"]==True:\r\n",
            "                   X=ab.fit_transform(X)\r\n",
            "                   \r\n",
            "               model=build_model(X.shape[1],unique_labels,model_parameters )     \r\n",
            "               \r\n",
            "               if validation_split<=0.0:\r\n",
            "                   \r\n",
            "                    model.fit_generator(generator=batch_generator(X, np_utils.to_categorical( y), batch_size, shuffle),\r\n",
            "                                        epochs=epochs,\r\n",
            "                                        steps_per_epoch=int(np.ceil(X.shape[0]/batch_size)),\r\n",
            "                                        verbose=verbose)                   \r\n",
            "                                      \r\n",
            "                     \r\n",
            "               else :\r\n",
            "                  \r\n",
            "                   x_train_oof, x_valid_oof, y_train_oof_nn, y_valid_oof_nn = cross_validation.train_test_split(\r\n",
            "                            X, y, test_size=validation_split, random_state=seed) \r\n",
            "\r\n",
            "                   callbacks = [\r\n",
            "                    EarlyStopping(\r\n",
            "                        monitor='val_loss', \r\n",
            "                        patience=stopping_rounds,\r\n",
            "                        verbose=verbose,\r\n",
            "                         mode='auto'),\r\n",
            "                    ModelCheckpoint(\r\n",
            "                        model_file, \r\n",
            "                        monitor='val_loss', \r\n",
            "                        save_best_only=True, \r\n",
            "                        verbose=verbose)\r\n",
            "                    ]\r\n",
            "                   \r\n",
            "                   model.fit_generator(generator=batch_generator(x_train_oof, np_utils.to_categorical( y_train_oof_nn), batch_size, shuffle),\r\n",
            "                                        epochs=epochs,\r\n",
            "                                        steps_per_epoch=int(np.ceil(x_train_oof.shape[0]/batch_size)),\r\n",
            "                                        validation_data=batch_generator(x_valid_oof, np_utils.to_categorical( y_valid_oof_nn), batch_size, False),\r\n",
            "                                        validation_steps=int(np.ceil(x_valid_oof.shape[0]/batch_size)),\r\n",
            "                                        callbacks=callbacks,\r\n",
            "                                        verbose=verbose)                   \r\n",
            "                                  \r\n",
            "            \r\n",
            "               joblib.dump((ab) , model_file+\".scaler\")                   \r\n",
            "               keras.models.save_model(model,model_file)\r\n",
            "               model=None\r\n",
            "               gc.collect()                     \r\n",
            "               if not os.path.isfile(model_file):\r\n",
            "                    raise Exception(\" %s model file could not be exported - check permissions ... \" % (model_file))\r\n",
            "                    \r\n",
            "            sys.exit(-1)# exit script\r\n",
            "        ################### predicting ###############            \r\n",
            "        else :\r\n",
            "            if not os.path.isfile(model_file):\r\n",
            "                raise Exception(\" %s model file could not be imported \" % (model_file))    \r\n",
            "            if not os.path.isfile(model_file+\".scaler\"):\r\n",
            "                raise Exception(\" %s sclaer file could not be imported \" % (model_file+\".scaler\"))                 \r\n",
            "            X,y=get_data(data_file, column) #load data\r\n",
            "            scaler=joblib.load(model_file+\".scaler\")\r\n",
            "            model=keras.models.load_model(model_file)\t\r\n",
            "            \r\n",
            "            if dense: #convert to dense - useful if the data does nto have high dimensionality .\r\n",
            "            #Also sklearn models are not optimzied for sparse data in tree-cased algos\r\n",
            "               X=X.toarray()\r\n",
            "               if \"use_log1p\" in model_parameters and model_parameters[\"use_log1p\"]==True:\r\n",
            "                   X[X<0]=0\r\n",
            "                   X=np.log1p(X)                  \r\n",
            "               if \"standardize\" in model_parameters and model_parameters[\"standardize\"]==True:\r\n",
            "                   X=scaler.transform(X)\r\n",
            "                   \r\n",
            "               preds=model.predict_proba(X,\r\n",
            "                                   verbose=verbose,\r\n",
            "                                   batch_size=batch_size)\r\n",
            "               np.savetxt(prediction_file, preds, delimiter=\",\", fmt='%.9f')               \r\n",
            "            \r\n",
            "            else :\r\n",
            "               if \"use_log1p\" in model_parameters and model_parameters[\"use_log1p\"]==True:\r\n",
            "                   X[X<0]=0\r\n",
            "                   X=csr_matrix(X).log1p()               \r\n",
            "               if \"standardize\" in model_parameters and model_parameters[\"standardize\"]==True:\r\n",
            "                   X=scaler.transform(X)\r\n",
            "\r\n",
            "                   \r\n",
            "                   \r\n",
            "               preds = model.predict_generator(generator=batch_generatorp(X, batch_size, False),\r\n",
            "                                               steps=int(np.ceil(X.shape[0]/batch_size))+1,\r\n",
            "                                               verbose=verbose)              \r\n",
            "               np.savetxt(prediction_file, preds, delimiter=\",\", fmt='%.9f')\r\n",
            "               \r\n",
            "            if not os.path.isfile(prediction_file):\r\n",
            "                raise Exception(\" %s prediction file could not be exported - check permissions ... \" % (prediction_file))             \r\n",
            "            sys.exit(-1)# exit script        \r\n",
            "                 \r\n",
            "if __name__==\"__main__\":\r\n",
            "  main()\r\n",
            "  \r\n",
            "\r\n",
            "\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1JAbs2brX9JY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 9. You can get 0.52879 if you average it 50-50 with this script , but bear in mind that the ids are sorted differently in the two files. So you might need to sort them before joining."
      ]
    }
  ]
}